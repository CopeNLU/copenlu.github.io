[{"authors":null,"categories":null,"content":"CopeNLU is a Natural Language Processing research group led by Isabelle Augenstein with a focus on researching methods for tasks that require a deep understanding of language, as opposed to shallow processing. We are affiliated with the Machine Learning Section at the Department of Computer Science, University of Copenhagen, as well as NLP at the University of Copenhagen. We are interested in core methodology research on, among others, learning with limited training data; as well as applications thereof to tasks such as fact checking, knowledge base population and question answering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://copenlu.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"CopeNLU is a Natural Language Processing research group led by Isabelle Augenstein with a focus on researching methods for tasks that require a deep understanding of language, as opposed to shallow processing. We are affiliated with the Machine Learning Section at the Department of Computer Science, University of Copenhagen, as well as NLP at the University of Copenhagen. We are interested in core methodology research on, among others, learning with limited training data; as well as applications thereof to tasks such as fact checking, knowledge base population and question answering.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://copenlu.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://copenlu.github.io/tutorial/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":"A postdoc position on question answering is available in the CopeNLU group at the Department of Computer Science, University of Copenhagen. The postdoc position is partly funded by a Megagon Labs faculty research award for the project `Subjective Question Answering´. The position is restricted to two years initially, after which a contract renewal is possible given funding availability. The current call is open until 16 April, and the preferred project start date is 16 May or soon afterwards.\nRead more about reasons to join us in our recent blog post. Before applying, you are very welcome get in touch to informally discuss further details about the position. We particularly welcome applications from researchers of underrepresented groups in NLP.\nThe call can be found here.\n","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"82ca2bbd93f9e1e9ad2c8232089804e7","permalink":"https://copenlu.github.io/talk/2019_02_postdoc/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/talk/2019_02_postdoc/","section":"talk","summary":"A postdoc position on question answering is available in the CopeNLU group at the Department of Computer Science, University of Copenhagen. The postdoc position is partly funded by a Megagon Labs faculty research award for the project `Subjective Question Answering´. The position is restricted to two years initially, after which a contract renewal is possible given funding availability. The current call is open until 16 April, and the preferred project start date is 16 May or soon afterwards.","tags":[],"title":"Postdoc Position on Question Answering","type":"talk"},{"authors":[],"categories":null,"content":"3 papers by CopeNLU authors are accepted to appear at NAACL 2019. Topics span from population of typological knowledge bases and weak supervision from disparate lexica to frame detection in online fora.\nA Probabilistic Generative Model of Linguistic Typology. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.\nCombining Disparate Sentiment Lexica with a Multi-View Variational Autoencoder. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell, Isabelle Augenstein.\nIssue Framing in Online Discussion Fora. Mareike Hartmann, Tallulah Jansen, Isabelle Augenstein, Anders Søgaard.\n","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"cadc8f4d9fa61b8034486ed3ea2a336a","permalink":"https://copenlu.github.io/talk/2019_02_naacl/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/talk/2019_02_naacl/","section":"talk","summary":"3 papers by CopeNLU authors are accepted to appear at NAACL 2019. Topics span from population of typological knowledge bases and weak supervision from disparate lexica to frame detection in online fora.\nA Probabilistic Generative Model of Linguistic Typology. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.\nCombining Disparate Sentiment Lexica with a Multi-View Variational Autoencoder. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell, Isabelle Augenstein.\nIssue Framing in Online Discussion Fora.","tags":[],"title":"3 Papers Accepted to NAACL 2019","type":"talk"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"568d0fcc90dddd21ecb612fb4a66e564","permalink":"https://copenlu.github.io/publication/2019_naacl_bjerva/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/publication/2019_naacl_bjerva/","section":"publication","summary":"In the Principles and Parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, besting several baselines on the task of predicting held-out features. Furthermore, we show that language representations pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":[],"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"bb5d04545be8a1e5502706ae3d78a674","permalink":"https://copenlu.github.io/publication/2019_naacl_hoyle/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/publication/2019_naacl_hoyle/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":[],"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Mareike Hartmann","Tallulah Jansen","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1550703600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550703600,"objectID":"e0a4b15989b58d07523ecd5a988a8020","permalink":"https://copenlu.github.io/publication/2019_naacl_hartmann/","publishdate":"2019-02-21T00:00:00+01:00","relpermalink":"/publication/2019_naacl_hartmann/","section":"publication","summary":"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","tags":[],"title":"Issue Framing in Online Discussion Fora","type":"publication"},{"authors":[],"categories":null,"content":"35 Marie Skłodowska-Curie PhD fellowships are available at the University of Copenhagen Faculty of Science via the TALENT program. The fellowship offers applicants employment as PhD fellows for a period of three years on a research topic of the fellow\u0026rsquo;s choice. The current call is open from 15 February to 1 April 2019 for a start date in Autumn 2019. Candidates are required to have completed a Master\u0026rsquo;s degree at the time of application, to not have resided in Denmark in the past 12 months, and to have a recent English language certificate.\nRead more about the fellowship on the program website, and more about reasons to join us in our recent blog post. Before applying, please get in touch to informally discuss possible topics. Note that eligibility criteria are enforced strictly, so if you are in doubt about whether you are eligible, please also enquire about this beforehand. When you\u0026rsquo;re ready, you can apply here.\nThe current call is the second invitation for proposals. Even though TALENT is a very competitive program, CopeNLU\u0026rsquo;s Pepa Atanasova holds one such fellowship, so we are in a good position to advise on application procedures.\n","date":1550185200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550185200,"objectID":"e1793d44156e097b09487ce2ead88d8a","permalink":"https://copenlu.github.io/talk/2019_02_phd/","publishdate":"2019-02-15T00:00:00+01:00","relpermalink":"/talk/2019_02_phd/","section":"talk","summary":"35 Marie Skłodowska-Curie PhD fellowships are available at the University of Copenhagen Faculty of Science via the TALENT program. The fellowship offers applicants employment as PhD fellows for a period of three years on a research topic of the fellow\u0026rsquo;s choice. The current call is open from 15 February to 1 April 2019 for a start date in Autumn 2019. Candidates are required to have completed a Master\u0026rsquo;s degree at the time of application, to not have resided in Denmark in the past 12 months, and to have a recent English language certificate.","tags":[],"title":"35 Marie Skłodowska-Curie PhD Fellowships Available","type":"talk"},{"authors":["Admin"],"categories":null,"content":" The University of Copenhagen is a great place if you\u0026rsquo;re both interested in high-quality NLP research (we\u0026rsquo;re ranked 2nd in Europe and 14th in the world on CSRankings for NLP) and a high quality of life (Denmark has consistently been ranked one of the three happiest countries in the world, and ranks equally highly on the quality of life). In addition to this, Copenhagen is a lively, bustling capital, while still being small enough that one can cycle nearly everwhere within a short amount of time. Intrigued? Read on!\n Life in Copenhagen NLP Research at the University of Copenhagen Applying for a PhD Doing a Postdoc CopeNLU Group Visiting the Group  Life in Copenhagen Copenhagen is lively and vibrant, yet comfortably-sized capital city by the North Sea on the border to Sweden with an oceanic climate. As it is located in Scandinavia, many of the corresponding stereotypes apply. Healthcare is socialised, public transport is cheap, working hours are short, taxes are high, but so are incomes. Denmark has been voted the happiest country in the world in 2016 and has kept a spot in the top three since, becoming a world-wide cultural phenomenon with many books published on the topic (some personal recommendations: \u0026ldquo;The Year of Living Danishly\u0026rdquo; by Helen Russell, and \u0026ldquo;The Almost Nearly Perfect People\u0026rdquo; by Michael Booth). Popular exports are hygge, beer and pork products.\nLiving in Copenhagen itself is a very relaxing experience. The city is very spacious compared to other capitals (think: London, New York, Tokyo), boasting large and open public spaces. Most people cycle to work and have a short commute, as it is easily possible to afford a city centre flat on an average salary. There are many things to do and explore in Copenhagen, from cozy bars and cafés to museums, the hipster district Vesterbro, the free town Christiania, and much more, as Copenhagen has incidentally also been voted the number 1 city to visit by Lonely Planet. The Copenhagen tourist board as well as the EMNLP 2017 local guide are excellent starting points for finding out more about what to do.\n         NLP Research at the University of Copenhagen The University of Copenhagen is currently home to three NLP faculty members and many postdocs and PhD students, who work on topics including natural language understanding, parsing, multi-lingual aspects, low-resource learning, machine translation and multi-modal learning.\nNLP at the University of Copenhagen as a whole is highly productive and internationally well-regarded. For instance, we are ranked 2nd in Europe and 14th in the world on CSRankings for NLP) since the beginning of NLP research activities at the University of Copenhagen in 2013, and were host to EMNLP 2017.\nApplying for a PhD PhD programs at the University of Copenhagen, and in Denmark in general, are fairly compact; students are expected to submit their thesis after three years, while an extension of one year can be granted. PhD students do not have to take courses during their studies, as they are expected to have completed a Master\u0026rsquo;s degree already. There are benefits and downsides to such a program structure, of course, and a more opinionated take on this can be found in this Quora post.\nThere are generally three routes to applying for a PhD:\n applying for a fully-funded position; applying with external funding; applying for an industrial PhD.  The first option is undoubtedly the most common one. Therein, a PhD fellowship is provided through a funded project, and the position is advertised on the KU job portal. The successful candidate becomes an employee at the University of Copenhagen, and in addition a PhD candidate registered with the PhD School. Depending on the funding source, a broad research topic is either already provided, or the topic is completely open.\nThe second option requires the PhD candidate to have already secured funding to cover living expenses in Denmark for a three-year period. Funding can, for instance, come from a governmental scholarship program such as CSC. The candidate then contacts the potential advisor, and given a positive resonse applies for admission to the PhD School afterwards.\nThe third option is something fairly special to Denmark \u0026ndash; PhD students can study towards their PhD, while working at a company at the same time roughly one day a week. These PhD projects tend to be more applied and the topic for them is defined based on an agreement between the PhD advisor and the company. Funding for such projects is either agreed upon with or without a candidate for the position. As with the other two options, the PhD student is enrolled at the Doctoral School.\nMore information on the enrollment process at the Science Faculty Doctoral School is listed here.\nDoing a Postdoc Postdoc positions are available for anything between one and four years. Applicants are expected to have submitted their PhD thesis by the time they start, but it is typically not necessary for them to have been formally awarded a PhD yet, if they have sufficient evidence that this is likely to happen in the near future.\nThe routes to getting a postdoc position are very similar to those for a PhD position:\n applying for a fully-funded position; applying with external funding; applying for an industrial PhD.  Funded postdoc positions are often advertised on the KU job portal, though 1-year postdoc contracts can be offered without open calls. The postdoc topic varies widely based on the funding source.\nIt is, furthermore, possible to start as a postdoc with already obtained funding, e.g. individual research fellowships. The topic for those is typically open and decided by the postdoc candidate. For some fellowship schemes, applications are made jointly with the host. Get in touch if you are interested in this option.\nLastly, industrial postdoc programs means a researcher works both at a university and at a company, roughly one day a week. The postdoc topic is typically more application-oriented and defined based on an agreement between the advisor and the company. Funding is either agreed upon with or without a candidate for the position.\nCopeNLU Group If you arrived on this page, you\u0026rsquo;ve likely already seen the rest of the CopeNLU website. We are a young research group at the Computer Science department at the University of Copenhagen, interested in natural language understanding. We are a very social and collaborative group, with weekly group meetings, breakfasts and reading groups, in addition to one-on-one adviser-advisee meetings. PhD students and postdocs are encouraged to not work on projects on their own, but rather form synergies with other group members based on common research interests.\nVisiting the Group We are always open to short or longer-term visitors to the group, but do not provide internships for Bachelor\u0026rsquo;s or Master\u0026rsquo;s students. Funding for visits can typically not be provided, with the expection of invited speakers who give a talk in the Copenhagen NLP meetup series.\n","date":1549062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549062000,"objectID":"dfe14b7cff1dd6a8394505f419e3da78","permalink":"https://copenlu.github.io/post/why-ucph/","publishdate":"2019-02-02T00:00:00+01:00","relpermalink":"/post/why-ucph/","section":"post","summary":"The University of Copenhagen is a great place if you're both interested in high-quality NLP research and a high quality of life.","tags":["Academic"],"title":"Interested in joining us at the University of Copenhagen?","type":"post"},{"authors":["Johannes Bjerva","Robert Östling","Maria Han Veiga","Jörg Tiedemann","Isabelle Augenstein"],"categories":null,"content":"","date":1548975600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548975600,"objectID":"77c14eca1f87d3b410f72a8c639228e9","permalink":"https://copenlu.github.io/publication/2019_cl_bjerva/","publishdate":"2019-02-01T00:00:00+01:00","relpermalink":"/publication/2019_cl_bjerva/","section":"publication","summary":"A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just like it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, while genetic relationships---a convenient benchmark used for evaluation in previous work---appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.","tags":[],"title":"What do Language Representations Really Represent?","type":"publication"},{"authors":["Sebastian Ruder","Joachim Bingel","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1548889200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548889200,"objectID":"c4469eafb02bfe1292a0bcad561bf951","permalink":"https://copenlu.github.io/publication/2019_aaai_ruder/","publishdate":"2019-01-31T00:00:00+01:00","relpermalink":"/publication/2019_aaai_ruder/","section":"publication","summary":"Multi-task learning (MTL) allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find (a) the layers or subspaces that benefit from sharing, (b) the appropriate amount of sharing, and (c) the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a latent multi-task architecture that jointly addresses (a)--(c). We present experiments on synthetic data and data from OntoNotes 5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15% average error reductions over common approaches to MTL.","tags":[],"title":"Latent multi-task architecture learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1548000421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548000421,"objectID":"95e81310bc3161eac19de3719bf77d94","permalink":"https://copenlu.github.io/people/isabelle/","publishdate":"2019-01-20T17:07:01+01:00","relpermalink":"/people/isabelle/","section":"people","summary":"","tags":["Members"],"title":"Isabelle Augenstein","type":"people"},{"authors":null,"categories":null,"content":"","date":1547914141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547914141,"objectID":"6b65be908a86e77256fbcbf4a3629746","permalink":"https://copenlu.github.io/people/johannes/","publishdate":"2019-01-19T17:09:01+01:00","relpermalink":"/people/johannes/","section":"people","summary":"","tags":["Members"],"title":"Johannes Bjerva","type":"people"},{"authors":null,"categories":null,"content":"","date":1547914021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547914021,"objectID":"9dec93e26a79a922ecb78073404d2edb","permalink":"https://copenlu.github.io/people/pepa/","publishdate":"2019-01-19T17:07:01+01:00","relpermalink":"/people/pepa/","section":"people","summary":"","tags":["Members"],"title":"Pepa Atanasova","type":"people"},{"authors":null,"categories":null,"content":"","date":1547741221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547741221,"objectID":"5e82376c2db939241c73c98ea3326c8b","permalink":"https://copenlu.github.io/people/giannis/","publishdate":"2019-01-17T17:07:01+01:00","relpermalink":"/people/giannis/","section":"people","summary":"","tags":["Members"],"title":"Giannis Bekoulis","type":"people"},{"authors":null,"categories":null,"content":"","date":1547654821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547654821,"objectID":"8f7ec6df3ddaddd23752a710340e1517","permalink":"https://copenlu.github.io/people/luna/","publishdate":"2019-01-16T17:07:01+01:00","relpermalink":"/people/luna/","section":"people","summary":"","tags":["Members"],"title":"Luna De Bruyne","type":"people"},{"authors":null,"categories":null,"content":"","date":1547568421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547568421,"objectID":"4a0551a8f7326bd863d21e9e474d8410","permalink":"https://copenlu.github.io/people/farhad/","publishdate":"2019-01-15T17:07:01+01:00","relpermalink":"/people/farhad/","section":"people","summary":"","tags":["Members"],"title":"Farhad Nooralahzadeh","type":"people"},{"authors":null,"categories":null,"content":"","date":1547568421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547568421,"objectID":"b96bcc0a756b3a2e3e6a041824ced7fb","permalink":"https://copenlu.github.io/people/xuan/","publishdate":"2019-01-15T17:07:01+01:00","relpermalink":"/people/xuan/","section":"people","summary":"","tags":["Members"],"title":"Zhong Xuan","type":"people"},{"authors":["Ana V. González-Garduño ","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1540940400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540940400,"objectID":"7be805f652a43c8d17f6d8767fc0c080","permalink":"https://copenlu.github.io/publication/2018_emnlp_gonzalez/","publishdate":"2018-10-31T00:00:00+01:00","relpermalink":"/publication/2018_emnlp_gonzalez/","section":"publication","summary":"The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks -- a task that amounts to question relevancy ranking -- involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google's search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.","tags":[],"title":"A strong baseline for question relevancy ranking","type":"publication"},{"authors":["Miryam de Lhoneux","Johannes Bjerva","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1540854000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540854000,"objectID":"11a2fc1d52386ca7a3d13ca4b2fea67f","permalink":"https://copenlu.github.io/publication/2018_emnlp_de-lhoneux/","publishdate":"2018-10-30T00:00:00+01:00","relpermalink":"/publication/2018_emnlp_de-lhoneux/","section":"publication","summary":"Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a parser on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.","tags":[],"title":"Parameter sharing between dependency parsers for related languages","type":"publication"},{"authors":["Yova Kementchedjhieva","Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"9dfc210ca496b9e0417e1f951d3f5044","permalink":"https://copenlu.github.io/publication/2018_sigmorphon_-kementchedjhieva/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/2018_sigmorphon_-kementchedjhieva/","section":"publication","summary":"This paper documents the Team Copenhagen system which placed first in the CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the 'inflection in context' task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.","tags":[],"title":" Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding","type":"publication"},{"authors":["Anders Søgaard","Miryam de Lhoneux","Isabelle Augenstein"],"categories":null,"content":"","date":1538258400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538258400,"objectID":"f3e4355ecd1db9a9ea4bfb28d4775004","permalink":"https://copenlu.github.io/publication/2018_blackbox_soegaard/","publishdate":"2018-09-30T00:00:00+02:00","relpermalink":"/publication/2018_blackbox_soegaard/","section":"publication","summary":"Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal. Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation. We also use punctuation ungrammatically for emphatic or creative purposes, or simply by mistake. We show that (a) dependency parsers are sensitive to both absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers without punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation. Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data.","tags":[],"title":"Nightmare at test time: How punctuation prevents parsers from generalizing","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://copenlu.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Dirk Weissenborn","Pasquale Minervini","Tim Dettmers","Isabelle Augenstein","Johannes Welbl","Tim Rocktäschel","Matko Bošnjak","Jeff Mitchell","Thomas Demeester","Pontus Stenetorp","Sebastian Riedel"],"categories":null,"content":"","date":1533074400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533074400,"objectID":"30a8402ff65b94561c394b6a22ec12f2","permalink":"https://copenlu.github.io/publication/2018_acl_weissenborn/","publishdate":"2018-08-01T00:00:00+02:00","relpermalink":"/publication/2018_acl_weissenborn/","section":"publication","summary":"Many Machine Reading and Natural Language Understanding tasks require reading supporting text in order to answer questions. For example, in Question Answering, the supporting text can be newswire or Wikipedia articles; in Natural Language Inference, premises can be seen as the supporting text and hypotheses as questions. Providing a set of useful primitives operating in a single framework of related tasks would allow for expressive modelling, and easier model comparison and replication. To that end, we present Jack the Reader (Jack), a framework for Machine Reading that allows for quick model prototyping by component reuse, evaluation of new models on existing datasets as well as integrating new datasets and applying them on a growing set of implemented baseline models. Jack is currently supporting (but not limited to) three tasks: Question Answering, Natural Language Inference, and Link Prediction. It is developed with the aim of increasing research efficiency and code reuse.","tags":[],"title":"Jack the Reader – A Machine Reading Framework","type":"publication"},{"authors":["Katharina Kann","Johannes Bjerva","Isabelle Augenstein","Barbara Plank","Anders Søgaard"],"categories":null,"content":"","date":1532988000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532988000,"objectID":"ecddfe04d32ca116b139cbaae21fda6c","permalink":"https://copenlu.github.io/publication/2018_deeplo_kann/","publishdate":"2018-07-31T00:00:00+02:00","relpermalink":"/publication/2018_deeplo_kann/","section":"publication","summary":"Neural part-of-speech (POS) taggers are known to not perform well with little training data. As a step towards overcoming this problem, we present an architecture for learning more robust neural POS taggers by jointly training a hierarchical, recurrent model and a recurrent characterbased sequence-to-sequence network supervised using an auxiliary objective. This way, we introduce stronger character-level supervision into the model, which enables better generalization to unseen words and provides regularization, making our encoding less prone to overfitting. We experiment with three auxiliary tasks: lemmatization, character-based word autoencoding, and character-based random string autoencoding. Experiments with minimal amounts of labeled data on 34 languages show that our new architecture outperforms a single-task baseline and, surprisingly, that, on average, raw text autoencoding can be as beneficial for lowresource POS tagging as using lemma information. Our neural POS tagger closes the gap to a state-of-the-art POS tagger (MarMoT) for low-resource scenarios by 43%, even outperforming it on languages with templatic morphology, e.g., Arabic, Hebrew, and Turkish, by some margin","tags":[],"title":"Character-level Supervision for Low-resource POS Tagging","type":"publication"},{"authors":["Isabelle Augenstein","Kris Cao","He He","Felix Hill","Spandana Gella","Jamie Kiros","Hongyuan Mei","Dipendra Misra"],"categories":null,"content":"","date":1532988000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532988000,"objectID":"6e6ca8c5d9f158ee98eede711a95f30f","permalink":"https://copenlu.github.io/publication/2018_repl4nlp_augenstein/","publishdate":"2018-07-31T00:00:00+02:00","relpermalink":"/publication/2018_repl4nlp_augenstein/","section":"publication","summary":"The ACL 2018 Workshop on Representation Learning for NLP (RepL4NLP) takes place on Friday, July 20, 2018 in Melbourne, Australia, immediately following the 56th Annual Meeting of the Association for Computational Linguistics (ACL). The workshop is generously sponsored by Facebook, Salesforce, ASAPP, DeepMind, Microsoft Research, and Naver. Repl4NLP is organised by Isabelle Augenstein, Kris Cao, He He, Felix Hill, Spandana Gella, Jamie Kiros, Hongyuan Mei and Dipendra Misra, and advised by Kyunghyun Cho, Edward Grefenstette, Karl Moritz Hermann and Laura Rimell. The 3rd Workshop on Representation Learning for NLP aims to continue the success of the 1st Workshop on Representation Learning for NLP, which received about 50 submissions and over 250 attendees and was the second most attended collocated event at ACL 2016 in Berlin, Germany after WMT; and the 2nd Workshop on Representation Learning for NLP at ACL 2017 in Vancouver, Canada. The workshop has a focus on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP.","tags":[],"title":"Proceedings of The Third Workshop on Representation Learning for NLP","type":"publication"},{"authors":["Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"04c9a44363b1ede24a1b169269f632e4","permalink":"https://copenlu.github.io/publication/2018_naacl_bjerva/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_bjerva/","section":"publication","summary":"A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task. For instance, although Norwegian Bokmal and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. We are also able to predict typological features in WALS with high accuracies, even for unseen language families.","tags":[],"title":"From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings","type":"publication"},{"authors":["Isabelle Augenstein","Sebastian Ruder","Anders Søgaard"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"59e919526653ad514493d33dd8423a61","permalink":"https://copenlu.github.io/publication/2018_naacl_augenstein/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_augenstein/","section":"publication","summary":"We combine multi-task learning and semisupervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis.","tags":[],"title":"Multi-Task Learning of Pairwise Sequence Classification Tasks over      Disparate Label Spaces","type":"publication"},{"authors":["Thomas Nyegaard-Signori","Casper Veistrup Helms","Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1527804000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527804000,"objectID":"d1aafea6345c3797374bf6e984b97818","permalink":"https://copenlu.github.io/publication/2018_naacl_nyegaard-signori/","publishdate":"2018-06-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_nyegaard-signori/","section":"publication","summary":"We take a multi-task learning approach to the shared Task 1 at SemEval-2018. The general idea concerning the model structure is to use as little external data as possible in order to preserve the task relatedness and reduce complexity. We employ multi-task learning with hard parameter sharing to exploit the relatedness between sub-tasks. As a base model, we use a standard recurrent neural network for both the classification and regression subtasks. Our system ranks 32nd out of 48 participants with a Pearson score of 0.557 in the first subtask, and 20th out of 35 in the fifth subtask with an accuracy score of 0.464.","tags":[],"title":"KU-MTL at SemEval-2018 Task 1: Multi-task Identification of Affect in Tweets","type":"publication"},{"authors":["Arkaitz Zubiaga","Elena Kochkina","Maria Liakata","Rob Procter","Michal Lukasik","Kalina Bontcheva","Trevor Cohn","Isabelle Augenstein"],"categories":null,"content":"","date":1519858800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519858800,"objectID":"16c50ca2bb654ef1b5375468d0a6cd8a","permalink":"https://copenlu.github.io/publication/2018_ipm_zubiaga/","publishdate":"2018-03-01T00:00:00+01:00","relpermalink":"/publication/2018_ipm_zubiaga/","section":"publication","summary":"Rumour stance classification, defined as classifying the stance of specific social media posts into one of supporting, denying, querying or commenting on an earlier post, is becoming of increasing interest to researchers. While most previous work has focused on using individual tweets as classifier inputs, here we report on the performance of sequential classifiers that exploit the discourse features inherent in social media interactions or 'conversational threads'. Testing the effectiveness of four sequential classifiers -- Hawkes Processes, Linear-Chain Conditional Random Fields (Linear CRF), Tree-Structured Conditional Random Fields (Tree CRF) and Long Short Term Memory networks (LSTM) -- on eight datasets associated with breaking news stories, and looking at different types of local and contextual features, our work sheds new light on the development of accurate stance classifiers. We show that sequential classifiers that exploit the use of discourse properties in social media conversations while using only local features, outperform non-sequential classifiers. Furthermore, we show that LSTM using a reduced set of features can outperform the other sequential classifiers; this performance is consistent across datasets and across types of stances. To conclude, our work also analyses the different features under study, identifying those that best help characterise and distinguish between stances, such as supporting tweets being more likely to be accompanied by evidence than denying tweets. We also set forth a number of directions for future research.","tags":[],"title":"Discourse-Aware Rumour Stance Classification in Social Media Using Sequential Classifiers","type":"publication"},{"authors":null,"categories":null,"content":"","date":1516550821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516550821,"objectID":"28b1f1dedf78ca52283f0864b4703960","permalink":"https://copenlu.github.io/people/ryan/","publishdate":"2018-01-21T17:07:01+01:00","relpermalink":"/people/ryan/","section":"people","summary":"","tags":["Affiliated"],"title":"Ryan Cotterell","type":"people"},{"authors":null,"categories":null,"content":"","date":1516464421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516464421,"objectID":"aa7eb934e88b2941f7b2ea1dbd8d2b9a","permalink":"https://copenlu.github.io/people/yova/","publishdate":"2018-01-20T17:07:01+01:00","relpermalink":"/people/yova/","section":"people","summary":"","tags":["Affiliated"],"title":"Yova Kementchedjhieva","type":"people"},{"authors":null,"categories":null,"content":"","date":1516378021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516378021,"objectID":"2aa98cf9e7d8b01f62de357efeda8999","permalink":"https://copenlu.github.io/people/ana/","publishdate":"2018-01-19T17:07:01+01:00","relpermalink":"/people/ana/","section":"people","summary":"","tags":["Affiliated"],"title":"Ana Valeria González","type":"people"},{"authors":null,"categories":null,"content":"","date":1516291621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516291621,"objectID":"c773ea7fff41710544da3ebb32d508e2","permalink":"https://copenlu.github.io/people/mareike/","publishdate":"2018-01-18T17:07:01+01:00","relpermalink":"/people/mareike/","section":"people","summary":"","tags":["Affiliated"],"title":"Mareike Hartmann","type":"people"},{"authors":["Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"375f4d44665380f3a0c98378eb3e263e","permalink":"https://copenlu.github.io/publication/2018_iwclul_bjerva/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/2018_iwclul_bjerva/","section":"publication","summary":"*English Abstract*: Although linguistic typology has a long history, computational approaches have only recently gained popularity. The use of distributed representations in computational linguistics has also become increasingly popular. A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another. Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing. In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks. We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations. We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline. *Finnish Abstract*: Vaikka kielitypologialla on pitkä historia, siihen liittyvät laskennalliset menetelmät ovat vasta viime aikoina saavuttaneet suosiota. Myös hajautettujen representaatioiden käyttö laskennallisessa kielitieteessä on tullut yhä suositummaksi. Viimeaikainen kehitys alalla on oppia kielestä hajautettu representaatio, joka esittää samankaltaiset kielet lähellä toisiaan. Vaikka kyseiset representaatiot nauttivatkin empiiristä menestystä, ei niitä ole huomattavasti tutkittu typologisesti. Tässä artikkelissa tutkitaan, ovatko tällaiset kielirepresentaatiot empiirisesti käyttökelpoisia uralilaisten kielten välisissä mallimuunnoksissa syvissä neuroverkoissa. Pyrkimällä ennustamaan piirteitä \textit{World Atlas of Language Structures}-tietokannassa tutkimme, mitä typologisia ominaisuuksia nämä representaatiot sisältävät. Keskityimme uralilaisiin kieliin ja huomasimme, että jotkin typologiset ominaisuudet voidaan automaattisesti päätellä tarkkuudella, joka ylittää selvästi vahvan perustason.","tags":[],"title":"Tracking Typological Traits of Uralic Languages in Distributed Language Representations","type":"publication"},{"authors":["Ed Collins","Isabelle Augenstein","Sebastian Riedel"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"7551d306de802e85bed4c73da927408b","permalink":"https://copenlu.github.io/publication/2017_conll_collins/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_conll_collins/","section":"publication","summary":"Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.","tags":[],"title":"A Supervised Approach to Extractive Summarisation of Scientific Papers","type":"publication"},{"authors":["Benjamin Riedel","Isabelle Augenstein","Georgios Spithourakis","Sebastian Riedel"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"2ceef4e10df7fe69e161e122c1289476","permalink":"https://copenlu.github.io/publication/2017_arxiv_riedel/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_arxiv_riedel/","section":"publication","summary":"Identifying public misinformation is a complicated and challenging task. An important part of checking the veracity of a specific claim is to evaluate the stance different news sources take towards the assertion. Automatic stance evaluation, i.e. stance detection, would arguably facilitate the process of fact checking. In this paper, we present our stance detection system which claimed third place in Stage 1 of the Fake News Challenge. Despite our straightforward approach, our system performs at a competitive level with the complex ensembles of the top two winning teams. We therefore propose our system as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance detection task.","tags":[],"title":"A simple but tough-to-beat baseline for the Fake News Challenge stance detection task","type":"publication"},{"authors":["Isabelle Augenstein","Leon Derczynski","Kalina Bontcheva"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"7687b70fc2497295c5eadce83e18dbcc","permalink":"https://copenlu.github.io/publication/2017_ipm_augenstein/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_ipm_augenstein/","section":"publication","summary":"Named Entity Recognition (NER) is a key NLP task, which is all the more challenging on Web and user-generated content with their diverse and continuously changing language. This paper aims to quantify how this diversity impacts state-of-the-art NER methods, by measuring named entity (NE) and context variability, feature sparsity, and their effects on precision and recall. In particular, our findings indicate that NER approaches struggle to generalise in diverse genres with limited training data. Unseen NEs, in particular, play an important role, which have a higher incidence in diverse genres such as social media than in more regular genres such as newswire. Coupled with a higher incidence of unseen features more generally and the lack of large training corpora, this leads to significantly lower F1 scores for diverse genres as compared to more regular ones. We also find that leading systems rely heavily on surface forms found in training data, having problems generalising beyond these, and offer explanations for this observation.","tags":[],"title":"Generalisation in Named Entity Recognition: A Quantitative Analysis","type":"publication"},{"authors":["Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"531bd207fb677a26642fb1f0fb61a1a0","permalink":"https://copenlu.github.io/publication/2017_acl_augenstein/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_acl_augenstein/","section":"publication","summary":"Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.","tags":[],"title":"Multi-Task Learning of Keyphrase Boundary Classification","type":"publication"},{"authors":["Isabelle Augenstein","Mrinal Das","Sebastian Riedel","Lakshmi Vikraman","Andrew McCallum"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"ac64c02d198f19b94c38dd746743353a","permalink":"https://copenlu.github.io/publication/2017_semeval_augenstein/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_semeval_augenstein/","section":"publication","summary":"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.","tags":[],"title":"SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications","type":"publication"},{"authors":["Elena Kochkina","Maria Liakata","Isabelle Augenstein"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"feb9b0fc3720c6ab93685d96db50c2f4","permalink":"https://copenlu.github.io/publication/2017_winlp_kochkina/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_winlp_kochkina/","section":"publication","summary":"Rumour stance classification is a task that involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose an LSTM-based sequential model that, through modelling the conversational structure of tweets, obtains state-of-theart accuracy on the SemEval-2017 RumourEval dataset.","tags":[],"title":"Sequential Approach to Rumour Stance Classification","type":"publication"},{"authors":["Elena Kochkina","Maria Liakata","Isabelle Augenstein"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"420b4382fe0eafec280fa592bc2a70b4","permalink":"https://copenlu.github.io/publication/2017_semeval_kochkina/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_semeval_kochkina/","section":"publication","summary":"This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.","tags":[],"title":"Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM","type":"publication"},{"authors":["Ziqi Zhang","Anna Lisa Gentile","Isabelle Augenstein","Eva Blomqvist","Fabio Ciravegna"],"categories":null,"content":"","date":1488322800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488322800,"objectID":"48dc2495a1f6b9cba1bcd037aed41069","permalink":"https://copenlu.github.io/publication/2017_swj_zhang/","publishdate":"2017-03-01T00:00:00+01:00","relpermalink":"/publication/2017_swj_zhang/","section":"publication","summary":"The Web of Data is currently undergoing an unprecedented level of growth thanks to the Linked Open Data effort. One escalated issue is the increasing level of heterogeneity in the published resources. This seriously hampers interoperability of Semantic Web applications. A decade of effort in the research of Ontology Alignment has contributed to a rich literature to solve such problems. However, existing methods can be still limited as 1) they primarily address concepts and entities while relations are less well-studied; 2) many build on the assumption of the ‘well-formedness’ of ontologies which is unnecessarily true in the domain of Linked Open Data; 3) few looked at schema heterogeneity from a single source, which is also a common issue particularly in very large Linked Dataset created automatically from heterogeneous resources, or integrated from multiple datasets. This article aims to address these issues with a domain- and language-independent and completely unsupervised method to align equivalent relations across schemata based on their shared instances. We propose a novel similarity measure able to cope with unbalanced population of schema elements, an unsupervised technique to automatically decide similarity threshold to assert equivalence for a pair of relations, and an unsupervised clustering process to discover groups of equivalent relations across different schemata. Although the method is designed for aligning relations within a single dataset, it can also be adapted for cross-dataset alignment where sameAs links between datasets have been established. Using three gold standards created based on DBpedia, we obtain encouraging results from a thorough evaluation involving four baseline similarity measures and over 15 comparative models based on variants of the proposed method. The proposed method makes significant improvement over baseline models in terms of F1 measure (mostly between 7% and 40%), and it always scores the highest precision and is also among the top performers in terms of recall. We also make public the datasets used in this work, which we believe make the largest collection of gold standards for evaluating relation alignment in the LOD context.","tags":[],"title":"An Unsupervised Data-driven Method to Discover Equivalent Relations in Large Linked Datasets","type":"publication"},{"authors":null,"categories":null,"content":"","date":1484928541,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484928541,"objectID":"aa55b41abc8e2441b932f86bfd7e67b1","permalink":"https://copenlu.github.io/people/sune/","publishdate":"2017-01-20T17:09:01+01:00","relpermalink":"/people/sune/","section":"people","summary":"","tags":["Alumni"],"title":"Sune Debel","type":"people"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"4acb79057f3b889f1ad90825bccc8b36","permalink":"https://copenlu.github.io/talk_backup/example/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/talk_backup/example/","section":"talk_backup","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk_backup"},{"authors":null,"categories":null,"content":"Information extraction is concerned with extracting information about entities, phrases and relations between them from text to populate knowledge bases, such as extracting \u0026ldquo;employee-at\u0026rdquo; relations. Within this context, we have worked on automatic knowledge base completion, knowledge base cleansing and detecting scientific keyphrases in text.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"dbf6be1f58ca6227cd1a2b0d1e198a14","permalink":"https://copenlu.github.io/project/knowledge-bases/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/knowledge-bases/","section":"project","summary":"Extract information about entities, phrases and relations between them from text to populate knowledge bases","tags":["nlu","knowledge-bases"],"title":"Knowledge Base Population","type":"project"},{"authors":null,"categories":null,"content":"Learning with limited labelled data is useful for small domains or languages with little resources. Methods we research to mitigate problems arising in these contexts include multi-task learning, weakly supervised and zero-shot learning.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"673da02c015e56b1ada0bfcd1fc5431c","permalink":"https://copenlu.github.io/project/limited-data/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/limited-data/","section":"project","summary":"Learning with limited labelled data, including multi-task learning, weakly supervised and zero-shot learning","tags":["lld","limited-data"],"title":"Learning with Limited Labelled Data","type":"project"},{"authors":null,"categories":null,"content":"Multi-lingual learning is concerned with training models to work well for multiple languages, including low-resource ones.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"c8405c441be49e9303825023c78eeadf","permalink":"https://copenlu.github.io/project/multilingual-learning/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/multilingual-learning/","section":"project","summary":"Training models to work well for multiple languages, including low-resource ones","tags":["lld","multilingual-learning"],"title":"Multilingual Learning","type":"project"},{"authors":null,"categories":null,"content":"Question answering is concerned with answer user questions, either in a closed-domain or open-domain setting automatically. We are interested in exploiting synergies between question answering and related tasks, such as framing entailment or relation extraction as question answering tasks. Further research interests are question answering in conversational settings, such as for chatbots.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"464368a48f0c4982ed7a6e6c606693ca","permalink":"https://copenlu.github.io/project/question-answering/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/question-answering/","section":"project","summary":"Answer questions automatically, including in conversational settings","tags":["nlu","question-answering"],"title":"Question Answering","type":"project"},{"authors":null,"categories":null,"content":"We are interested in studying method to determine the attitude expressed in a text towards a topic (stance detection), such as determining if a tweet expresses a positive, negative or neutral stance towards a political entity. One additional challenge we are exploring is stance detection in a conversational context, where the stance depends on the context of the conversation. Fact checking using textual data can be framed very similarly, namely as if an evidence document agrees with, disagrees with or is topically unrelated to a headline or claim.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"9b262f5564a2254298043b81d48f5aa5","permalink":"https://copenlu.github.io/project/fact-checking/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/fact-checking/","section":"project","summary":"Determine the attitude expressed in a text towards a topic, and use this for automatic evidence-based fact checking","tags":["nlu","fact-checking"],"title":"Stance Detection and Fact Checking","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441058400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441058400,"objectID":"02ddcc34f9b380308becfdd12994b2f4","permalink":"https://copenlu.github.io/publication_backup/person-re-id/","publishdate":"2015-09-01T00:00:00+02:00","relpermalink":"/publication_backup/person-re-id/","section":"publication_backup","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication_backup"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372629600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372629600,"objectID":"0af71cf2c5a7a78c0b9a5906512f8538","permalink":"https://copenlu.github.io/publication_backup/clothing-search/","publishdate":"2013-07-01T00:00:00+02:00","relpermalink":"/publication_backup/clothing-search/","section":"publication_backup","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication_backup"}]