[{"authors":null,"categories":null,"content":"CopeNLU is a Natural Language Processing research group led by Isabelle Augenstein with a focus on researching methods for tasks that require a deep understanding of language, as opposed to shallow processing. We are affiliated with the Machine Learning Section at the Department of Computer Science, University of Copenhagen, as well as NLP at the University of Copenhagen. We are interested in core methodology research on, among others, learning with limited training data; as well as applications thereof to tasks such as fact checking, knowledge base population and question answering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://copenlu.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"CopeNLU is a Natural Language Processing research group led by Isabelle Augenstein with a focus on researching methods for tasks that require a deep understanding of language, as opposed to shallow processing. We are affiliated with the Machine Learning Section at the Department of Computer Science, University of Copenhagen, as well as NLP at the University of Copenhagen. We are interested in core methodology research on, among others, learning with limited training data; as well as applications thereof to tasks such as fact checking, knowledge base population and question answering.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://copenlu.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://copenlu.github.io/tutorial/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Nils Rethmeier","Vageesh Kumar Saxena","Isabelle Augenstein"],"categories":null,"content":"","date":1575241200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575241200,"objectID":"d8ddd63e083c4f52eacee172db9ec77c","permalink":"https://copenlu.github.io/publication/2019_arxiv_rethmeier/","publishdate":"2019-12-02T00:00:00+01:00","relpermalink":"/publication/2019_arxiv_rethmeier/","section":"publication","summary":"While state-of-the-art NLP explainability (XAI) methods focus on supervised, per-instance end or diagnostic probing task evaluation[4, 2, 10], this is insufficient to interpret and quantify model knowledge transfer during (un-) supervised training. By instead expressing each neuron as an interpretable token-activation distribution collected over many instances, one can quantify and guide visual exploration of neuron-knowledge change between model training stages to analyze transfer beyond probing tasks and the per-instance level. This allows one to analyze: (RQ1) how neurons abstract knowledge during unsupervised pretraining; (RQ2) how pretrained neurons zero-shot transfer knowledge to new domain data; and (RQ3) how supervised tasks reorder pretrained neuron knowledge abstractions. Since the meaningfulness of XAI methods is hard to quantify [11, 4], we analyze three example learning setups (RQ1-3) to empirically verify that our method (TX-Ray): identifies transfer (ir-)relevant neurons for pruning (RQ3), and that its transfer metrics coincide with traditional measures like perplexity (RQ1). We also find, that TX-Ray guided pruning of supervision (ir-)relevant neuron-knowledge (RQ3) can identify `lottery ticket'-like [9, 40] neurons that drive model performance and robustness. Upon inspecting pruned neurons, we find that task-relevant neuron-knowledge (`tickets'), appear (over-)fit, while task-irrelevant neurons lower overfitting, i.e. TX-Ray identifies neurons that generalize, transfer or specialize model-knowledge [25]. Finally, through RQ1-3, we find that TX-Ray helps to explore and quantify dynamics of (continual) knowledge transfer and that it can shed light on neuron-knowledge specialization and generalization, to complement (costly) supervised probing task procurement and established `summary' statistics like perplexity, ROC or F scores.","tags":[],"title":"TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised NLP","type":"publication"},{"authors":["Luna De Bruyne","Pepa Atanasova","Isabelle Augenstein"],"categories":null,"content":"","date":1574204400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574204400,"objectID":"67b79766167d12b8a6ef6eb727e79b6e","permalink":"https://copenlu.github.io/publication/2019_arxiv_debruyne/","publishdate":"2019-11-20T00:00:00+01:00","relpermalink":"/publication/2019_arxiv_debruyne/","section":"publication","summary":"Emotion lexica are commonly used resources to combat data poverty in automatic emotion detection. However, methodological issues emerge when employing them: lexica are often not very extensive, and the way they are constructed can vary widely -- from lab conditions to crowdsourced approaches and distant supervision. Furthermore, both categorical frameworks and dimensional frameworks coexist, in which theorists provide many different sets of categorical labels or dimensional axes. The heterogenous nature of the resulting emotion detection resources results in a need for a unified approach to utilising them. This paper contributes to the field of emotion analysis in NLP by a) presenting the first study to unify existing emotion detection resources automatically and thus learn more about the relationships between them; b) exploring the use of existing lexica for the above-mentioned task; c) presenting an approach to automatically combining emotion lexica, namely by a multi-view variational auto-encoder (VAE), which facilitates the mapping of datasets into a joint emotion label space. We test the utility of joint emotion lexica by using them as additional features in state-of-the art emotion detection models. Our overall findings are that emotion lexica can offer complementary information to even extremely large pre-trained models such as BERT. The performance of our models is comparable to state-of-the art models that are specifically engineered for certain datasets, and even outperform the state-of-the art on four datasets.","tags":[],"title":"Joint Emotion Label Space Modelling for Affect Lexica","type":"publication"},{"authors":["Johannes Bjerva","Wouter Kouw","Isabelle Augenstein"],"categories":null,"content":"","date":1573426800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573426800,"objectID":"44ab0384bb61b068fbc09fc327cdd196","permalink":"https://copenlu.github.io/publication/2020_aaai_bjerva_kouw/","publishdate":"2019-11-11T00:00:00+01:00","relpermalink":"/publication/2020_aaai_bjerva_kouw/","section":"publication","summary":"Language evolves over time in many ways relevant to natural language processing tasks. For example, recent occurrences of tokens 'BERT' and 'ELMO' in publications refer to neural network architectures rather than persons. This type of temporal signal is typically overlooked, but is important if one aims to deploy a machine learning model over an extended period of time. In particular, language evolution causes data drift between time-steps in sequential decision-making tasks. Examples of such tasks include prediction of paper acceptance for yearly conferences (regular intervals) or author stance prediction for rumours on Twitter (irregular intervals). Inspired by successes in computer vision, we tackle data drift by sequentially aligning learned representations. We evaluate on three challenging tasks varying in terms of time-scales, linguistic units, and domains. These tasks show our method outperforming several strong baselines, including using all available data. We argue that, due to its low computational expense, sequential alignment is a practical solution to dealing with language evolution.","tags":[],"title":"Back to the Future -- Sequential Alignment of Text Representations","type":"publication"},{"authors":[],"categories":null,"content":"4 papers by CopeNLU authors are to be presented at EMNLP 2019 and co-located events, on fact checking and disinformation, as well as on multi-task and multi-lingual learning.\nMultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen. In Proceedings of EMNLP-IJCNLP 2019.\nMapping (Dis-)Information Flow about the MH17 Plane Crash. Mareike Hartmann, Yevgeniy Golovchenko, Isabelle Augenstein. In Proceedings of the 2019 Workshop on NLP4IF: censorship, disinformation, and propaganda (NLP4IF at EMNLP-IJCNLP 2019).\nX-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension. Mostafa Abdou, Cezar Sas, Rahul Aralikatte, Isabelle Augenstein, Anders Søgaard. In Proceedings of the 2nd Workshop on Deep Learning for Low-Resource NLP (DeepLo at EMNLP-IJCNLP 2019).\nTransductive Auxiliary Task Self-Training for Neural Multi-Task Models. Johannes Bjerva, Katharina Kann, Isabelle Augenstein. In Proceedings of the 2nd Workshop on Deep Learning for Low-Resource NLP (DeepLo at EMNLP-IJCNLP 2019).\n","date":1572562800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572562800,"objectID":"66a85a74e8c590681d82ae2d34cbf254","permalink":"https://copenlu.github.io/talk/2019_11_emnlp/","publishdate":"2019-11-01T00:00:00+01:00","relpermalink":"/talk/2019_11_emnlp/","section":"talk","summary":"4 papers by CopeNLU authors are to be presented at EMNLP 2019 and co-located events, on fact checking and disinformation, as well as on multi-task and multi-lingual learning.\nMultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen. In Proceedings of EMNLP-IJCNLP 2019.\nMapping (Dis-)Information Flow about the MH17 Plane Crash. Mareike Hartmann, Yevgeniy Golovchenko, Isabelle Augenstein.","tags":[],"title":"4 papers to be presented at EMNLP 2019","type":"talk"},{"authors":[],"categories":null,"content":"A PhD fellowship on gender bias and stance detection is available in CopeNLU. The position is funded by a basic research project on the same topic funded by the Independent Research Fund Denmark (DFF). The fellowship offers the applicant employment as a PhD fellow for a period of three years. The successful candidate will be advised by Isabelle Augenstein and co-advised by Ryan Cotterell (ETH Zurich). Funding for bilateral visits is available. The call is open until 1 December 2019 for a start in Spring 2020.\nCandidates are expected to hold a Master\u0026rsquo;s degree in computer science or a related relevant area or be near completion for one. There are no restrictions on citizenship, and a language certificate is not required at application time. Candidates from traditionally underrepresented minorities in natural language processing are particularly encouraged to apply.\nRead more about reasons to join us in our recent blog post. Before applying, you are welcome to get in touch to informally if you have questions about the call. When you\u0026rsquo;re ready, you can apply here.\nIf you are interested in joining CopeNLU as a PhD student, but would prefer to research a different topic, please keep an eye on the TALENT PhD fellowship website, which offers successful candidates the opportunity to propose any research topic. The next call for this is expected to be published on 2 January 2020 for a start in Summer 2020. Note that even though TALENT is a very competitive program, CopeNLU\u0026rsquo;s Pepa Atanasova and Dustin Wright hold such fellowships, so we are in a good position to advise on application procedures.\n","date":1572476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572476400,"objectID":"bd8aa808e377f511003def093823d93b","permalink":"https://copenlu.github.io/talk/2019_10_phd/","publishdate":"2019-10-31T00:00:00+01:00","relpermalink":"/talk/2019_10_phd/","section":"talk","summary":"A PhD fellowship on gender bias and stance detection is available in CopeNLU. The position is funded by a basic research project on the same topic funded by the Independent Research Fund Denmark (DFF). The fellowship offers the applicant employment as a PhD fellow for a period of three years. The successful candidate will be advised by Isabelle Augenstein and co-advised by Ryan Cotterell (ETH Zurich). Funding for bilateral visits is available.","tags":[],"title":"PhD Fellowship on Gender Bias and Stance Detection available","type":"talk"},{"authors":[],"categories":null,"content":"25 Marie Skłodowska-Curie PhD fellowships are available at the University of Copenhagen Faculty of Science via the TALENT program. The fellowship offers applicants employment as PhD fellows for a period of three years on a research topic of the fellow\u0026rsquo;s choice. The current call is open from 15 August to 1 October 2019 for a start date in Spring 2020. Candidates are required to have completed a Master\u0026rsquo;s degree at the time of application, to not have resided in Denmark in the past 12 months, and to have a recent English language certificate.\nRead more about the fellowship on the program website, and more about reasons to join us in our recent blog post. Before applying, please get in touch to informally discuss possible topics. Note that eligibility criteria are enforced strictly, so if you are in doubt about whether you are eligible, please also enquire about this beforehand. When you\u0026rsquo;re ready, you can apply here.\nThe current call is the second invitation for proposals. Even though TALENT is a very competitive program, CopeNLU\u0026rsquo;s Pepa Atanasova holds one such fellowship, so we are in a good position to advise on application procedures.\n","date":1565820000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565820000,"objectID":"5318546b8998274c404ebbcd241ca33e","permalink":"https://copenlu.github.io/talk/2019_08_phd/","publishdate":"2019-08-15T00:00:00+02:00","relpermalink":"/talk/2019_08_phd/","section":"talk","summary":"25 Marie Skłodowska-Curie PhD fellowships are available at the University of Copenhagen Faculty of Science via the TALENT program. The fellowship offers applicants employment as PhD fellows for a period of three years on a research topic of the fellow\u0026rsquo;s choice. The current call is open from 15 August to 1 October 2019 for a start date in Spring 2020. Candidates are required to have completed a Master\u0026rsquo;s degree at the time of application, to not have resided in Denmark in the past 12 months, and to have a recent English language certificate.","tags":[],"title":"25 Marie Skłodowska-Curie PhD Fellowships Available","type":"talk"},{"authors":["Isabelle Augenstein","Christina Lioma","Dongsheng Wang","Lucas Chaves Lima","Casper Hansen","Christian Hansen","Jakob Grue Simonsen"],"categories":null,"content":"","date":1565733600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565733600,"objectID":"4be9f04cad3214b7e117e486001cf0b2","permalink":"https://copenlu.github.io/publication/2019_emnlp_augenstein/","publishdate":"2019-08-14T00:00:00+02:00","relpermalink":"/publication/2019_emnlp_augenstein/","section":"publication","summary":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","tags":[],"title":"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims","type":"publication"},{"authors":["Mareike Hartmann","Yevgeniy Golovchenko","Isabelle Augenstein"],"categories":null,"content":"","date":1565647200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565647200,"objectID":"7f017aedb120ed83bd90a0ce329a5549","permalink":"https://copenlu.github.io/publication/2019_nlp4if_hartmann/","publishdate":"2019-08-13T00:00:00+02:00","relpermalink":"/publication/2019_nlp4if_hartmann/","section":"publication","summary":"Digital media enables not only fast sharing of information, but also disinformation. One prominent case of an event leading to circulation of disinformation on social media is the MH17 plane crash. Studies analysing the spread of information about this event on Twitter have focused on small, manually annotated datasets, or used proxys for data annotation. In this work, we examine to what extent text classifiers can be used to label data for subsequent content analysis, in particular we focus on predicting pro-Russian and pro-Ukrainian Twitter content related to the MH17 plane crash. Even though we find that a neural classifier improves over a hashtag based baseline, labeling pro-Russian and pro-Ukrainian content with high precision remains a challenging problem. We provide an error analysis underlining the difficulty of the task and identify factors that might help improve classification in future work. Finally, we show how the classifier can facilitate the annotation task for human annotators.","tags":[],"title":"Mapping (Dis-)Information Flow about the MH17 Plane Crash","type":"publication"},{"authors":["Joachim Bingel","Victor Petrén Bach Hansen","Ana Valeria Gonzalez","Paweł Budzianowski","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1565560800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565560800,"objectID":"e60966e2b6babcb61c965c98311808af","permalink":"https://copenlu.github.io/publication/2019_convai_bingel/","publishdate":"2019-08-12T00:00:00+02:00","relpermalink":"/publication/2019_convai_bingel/","section":"publication","summary":"Task oriented dialogue systems rely heavily on specialized dialogue state tracking (DST) modules for dynamically predicting user intent throughout the conversation. State-of-the-art DST models are typically trained in a supervised manner from manual annotations at the turn level. However, these annotations are costly to obtain, which makes it difficult to create accurate dialogue systems for new domains. To address these limitations, we propose a method, based on reinforcement learning, for transferring DST models to new domains without turn-level supervision. Across several domains, our experiments show that this method quickly adapts off-the-shelf models to new domains and performs on par with models trained with turn-level supervision. We also show our method can improve models trained using turn-level supervision by subsequent fine-tuning optimization toward dialog-level rewards.","tags":[],"title":"Domain Transfer in Dialogue Systems without Turn-Level Supervision","type":"publication"},{"authors":["Ana Valeria Gonzalez","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1565560800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565560800,"objectID":"5ffe12cab0c4cb4dac164965635cb813","permalink":"https://copenlu.github.io/publication/2019_convai_gonzalez/","publishdate":"2019-08-12T00:00:00+02:00","relpermalink":"/publication/2019_convai_gonzalez/","section":"publication","summary":"Task oriented dialogue systems rely heavily on specialized dialogue state tracking (DST) modules for dynamically predicting user intent throughout the conversation. State-of-the-art DST models are typically trained in a supervised manner from manual annotations at the turn level. However, these annotations are costly to obtain, which makes it difficult to create accurate dialogue systems for new domains. To address these limitations, we propose a method, based on reinforcement learning, for transferring DST models to new domains without turn-level supervision. Across several domains, our experiments show that this method quickly adapts off-the-shelf models to new domains and performs on par with models trained with turn-level supervision. We also show our method can improve models trained using turn-level supervision by subsequent fine-tuning optimization toward dialog-level rewards.","tags":[],"title":"Retrieval-Based Goal-Oriented Dialogue Generation","type":"publication"},{"authors":["Mostafa Abdou","Cezar Sas","Rahul Aralikatte","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1565560800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565560800,"objectID":"2a01314b332ff8ab8508ff83d8cb49c9","permalink":"https://copenlu.github.io/publication/2019_deeplo_abdou/","publishdate":"2019-08-12T00:00:00+02:00","relpermalink":"/publication/2019_deeplo_abdou/","section":"publication","summary":"Although the vast majority of knowledge bases KBs are heavily biased towards English, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.","tags":[],"title":"X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension","type":"publication"},{"authors":["Johannes Bjerva","Katharina Kann","Isabelle Augenstein"],"categories":null,"content":"","date":1565474400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565474400,"objectID":"a5054ee59833f81fcfebde72b86819ad","permalink":"https://copenlu.github.io/publication/2019_deeplo_bjerva/","publishdate":"2019-08-11T00:00:00+02:00","relpermalink":"/publication/2019_deeplo_bjerva/","section":"publication","summary":"Multi-task learning and self-training are two common ways to improve a machine learning model's performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training: training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the model has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56% over the pure multi-task model for dependency relation tagging and by up to 13.03% for semantic tagging.","tags":[],"title":"Transductive Auxiliary Task Self-Training for Neural Multi-Task Models","type":"publication"},{"authors":[],"categories":null,"content":"2 papers by CopeNLU authors are accepted to appear at ACL 2019. One paper is on uncovering probabilistic implications in typological knowledge bases, following up from our NAACL 2019 paper on generative linguistic typology; whereas the other one is on unsupervised discovery of gendered language, utilising the multi-view autoencoder introduced in our NAACL 219 paper.\nUncovering Probabilistic Implications in Typological Knowledge Bases. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.\nUnsupervised Discovery of Gendered Language through Latent-Variable Modeling. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Isabelle Augenstein, Ryan Cotterell.\n","date":1557784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557784800,"objectID":"99bb0884709b5538d1fdb51c792a32c5","permalink":"https://copenlu.github.io/talk/2019_05_acl/","publishdate":"2019-05-14T00:00:00+02:00","relpermalink":"/talk/2019_05_acl/","section":"talk","summary":"2 papers by CopeNLU authors are accepted to appear at ACL 2019. One paper is on uncovering probabilistic implications in typological knowledge bases, following up from our NAACL 2019 paper on generative linguistic typology; whereas the other one is on unsupervised discovery of gendered language, utilising the multi-view autoencoder introduced in our NAACL 219 paper.\nUncovering Probabilistic Implications in Typological Knowledge Bases. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.","tags":[],"title":"2 Papers Accepted to ACL 2019","type":"talk"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1557784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557784800,"objectID":"0b58a94448025cc2412612d8fde6f580","permalink":"https://copenlu.github.io/publication/2019_acl_bjerva/","publishdate":"2019-05-14T00:00:00+02:00","relpermalink":"/publication/2019_acl_bjerva/","section":"publication","summary":"The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.","tags":[],"title":"Uncovering Probabilistic Implications in Typological Knowledge Bases","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1557784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557784800,"objectID":"360c04b943d2ac91d6031552ad5845fb","permalink":"https://copenlu.github.io/publication/2019_acl_hoyle/","publishdate":"2019-05-14T00:00:00+02:00","relpermalink":"/publication/2019_acl_hoyle/","section":"publication","summary":"Studying to what degree the language we use is gender-specific has long been an area of interest in socio-linguistics. Studies have explored, for instance, the speech of male and female characters in film, or gendered language used when describing male versus female politicians. In this paper, we aim not to merely analyze this phenomenon qualitatively, but instead to quantify the degree to which language used to describe men and women is different, and moreover, different in a positive or negative way. We propose a novel generative latent-variable model, to be trained on a large corpus, that jointly represents adjective (or verb) choice with its sentiment given the natural gender of the head (or dependent) noun. We find that there are significant differences between how male and female nouns are described, which are in line with common gender stereotypes: Positive adjectives used to describe women are more likely to be related to a person's body than adjectives describing men.","tags":[],"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","type":"publication"},{"authors":["Isabelle Augenstein","Spandana Gella","Sebastian Ruder","Katharina Kann","Burcu Can","Alexis Conneau","Johannes Welbl","Xian Ren","Marek Rei"],"categories":null,"content":"","date":1555711200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555711200,"objectID":"c7686844bbcb0a906aeef2426354f008","permalink":"https://copenlu.github.io/publication/2019_repl4nlp_augenstein/","publishdate":"2019-04-20T00:00:00+02:00","relpermalink":"/publication/2019_repl4nlp_augenstein/","section":"publication","summary":"The 4th Workshop on Representation Learning for NLP (RepL4NLP) will be hosted by ACL 2019 and held on 2 August 2019. The workshop is being organised by Isabelle Augenstein, Spandana Gella, Sebastian Ruder, Katharina Kann, Burcu Can, Alexis Conneau, Johannes Welbl, Xian Ren and Marek Rei; and advised by Kyunghyun Cho, Edward Grefenstette, Karl Moritz Hermann, Chris Dyer and Laura Rimell. The workshop is organised by the ACL Special Interest Group on Representation Learning (SIGREP) and receives generous sponsorship from Facebook AI Research, Amazon, and Naver. The 4th Workshop on Representation Learning for NLP aims to continue the success of the 1st Workshop on Representation Learning for NLP (about 50 submissions and over 250 attendees; second most attended collocated event at ACL’16 after WMT), 2nd Workshop on Representation Learning for NLP and 3rd Workshop on Representation Learning for NLP. The workshop was introduced as a synthesis of several years of independent *CL workshops focusing on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP.","tags":[],"title":"Proceedings of The Fourth Workshop on Representation Learning for NLP","type":"publication"},{"authors":[],"categories":null,"content":"3 papers by CopeNLU authors are accepted to appear at NAACL 2019. Topics span from population of typological knowledge bases and weak supervision from disparate lexica to frame detection in online fora.\nA Probabilistic Generative Model of Linguistic Typology. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.\nCombining Disparate Sentiment Lexica with a Multi-View Variational Autoencoder. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell, Isabelle Augenstein.\nIssue Framing in Online Discussion Fora. Mareike Hartmann, Tallulah Jansen, Isabelle Augenstein, Anders Søgaard.\n","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"cadc8f4d9fa61b8034486ed3ea2a336a","permalink":"https://copenlu.github.io/talk/2019_02_naacl/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/talk/2019_02_naacl/","section":"talk","summary":"3 papers by CopeNLU authors are accepted to appear at NAACL 2019. Topics span from population of typological knowledge bases and weak supervision from disparate lexica to frame detection in online fora.\nA Probabilistic Generative Model of Linguistic Typology. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.\nCombining Disparate Sentiment Lexica with a Multi-View Variational Autoencoder. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell, Isabelle Augenstein.\nIssue Framing in Online Discussion Fora.","tags":[],"title":"3 Papers Accepted to NAACL 2019","type":"talk"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"568d0fcc90dddd21ecb612fb4a66e564","permalink":"https://copenlu.github.io/publication/2019_naacl_bjerva/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/publication/2019_naacl_bjerva/","section":"publication","summary":"In the Principles and Parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, besting several baselines on the task of predicting held-out features. Furthermore, we show that language representations pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":[],"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1550790000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550790000,"objectID":"bb5d04545be8a1e5502706ae3d78a674","permalink":"https://copenlu.github.io/publication/2019_naacl_hoyle/","publishdate":"2019-02-22T00:00:00+01:00","relpermalink":"/publication/2019_naacl_hoyle/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":[],"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Mareike Hartmann","Tallulah Jansen","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1550703600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550703600,"objectID":"e0a4b15989b58d07523ecd5a988a8020","permalink":"https://copenlu.github.io/publication/2019_naacl_hartmann/","publishdate":"2019-02-21T00:00:00+01:00","relpermalink":"/publication/2019_naacl_hartmann/","section":"publication","summary":"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","tags":[],"title":"Issue Framing in Online Discussion Fora","type":"publication"},{"authors":["Admin"],"categories":null,"content":" The University of Copenhagen is a great place if you\u0026rsquo;re both interested in high-quality NLP research (we\u0026rsquo;re ranked 2nd in Europe and 14th in the world on CSRankings for NLP) and a high quality of life (Denmark has consistently been ranked one of the three happiest countries in the world, and ranks equally highly on the quality of life). In addition to this, Copenhagen is a lively, bustling capital, while still being small enough that one can cycle nearly everwhere within a short amount of time. Intrigued? Read on!\n Life in Copenhagen NLP Research at the University of Copenhagen Applying for a PhD Doing a Postdoc CopeNLU Group Visiting the Group  Life in Copenhagen Copenhagen is lively and vibrant, yet comfortably-sized capital city by the North Sea on the border to Sweden with an oceanic climate. As it is located in Scandinavia, many of the corresponding stereotypes apply. Healthcare is socialised, public transport is cheap, working hours are short, taxes are high, but so are incomes. Denmark has been voted the happiest country in the world in 2016 and has kept a spot in the top three since, becoming a world-wide cultural phenomenon with many books published on the topic (some personal recommendations: \u0026ldquo;The Year of Living Danishly\u0026rdquo; by Helen Russell, and \u0026ldquo;The Almost Nearly Perfect People\u0026rdquo; by Michael Booth). Popular exports are hygge, beer and pork products.\nLiving in Copenhagen itself is a very relaxing experience. The city is very spacious compared to other capitals (think: London, New York, Tokyo), boasting large and open public spaces. Most people cycle to work and have a short commute, as it is easily possible to afford a city centre flat on an average salary. There are many things to do and explore in Copenhagen, from cozy bars and cafés to museums, the hipster district Vesterbro, the free town Christiania, and much more, as Copenhagen has incidentally also been voted the number 1 city to visit by Lonely Planet. The Copenhagen tourist board as well as the EMNLP 2017 local guide are excellent starting points for finding out more about what to do.\n         NLP Research at the University of Copenhagen The University of Copenhagen is currently home to three NLP faculty members and many postdocs and PhD students, who work on topics including natural language understanding, parsing, multi-lingual aspects, low-resource learning, machine translation and multi-modal learning.\nNLP at the University of Copenhagen as a whole is highly productive and internationally well-regarded. For instance, we are ranked 2nd in Europe and 14th in the world on CSRankings for NLP) since the beginning of NLP research activities at the University of Copenhagen in 2013, and were host to EMNLP 2017.\nApplying for a PhD PhD programs at the University of Copenhagen, and in Denmark in general, are fairly compact; students are expected to submit their thesis after three years, while an extension of one year can be granted. PhD students do not have to take courses during their studies, as they are expected to have completed a Master\u0026rsquo;s degree already. There are benefits and downsides to such a program structure, of course, and a more opinionated take on this can be found in this Quora post.\nThere are generally three routes to applying for a PhD:\n applying for a fully-funded position; applying with external funding; applying for an industrial PhD.  The first option is undoubtedly the most common one. Therein, a PhD fellowship is provided through a funded project, and the position is advertised on the KU job portal. The successful candidate becomes an employee at the University of Copenhagen, and in addition a PhD candidate registered with the PhD School. Depending on the funding source, a broad research topic is either already provided, or the topic is completely open.\nThe second option requires the PhD candidate to have already secured funding to cover living expenses in Denmark for a three-year period. Funding can, for instance, come from a governmental scholarship program such as CSC. The candidate then contacts the potential advisor, and given a positive resonse applies for admission to the PhD School afterwards.\nThe third option is something fairly special to Denmark \u0026ndash; PhD students can study towards their PhD, while working at a company at the same time roughly one day a week. These PhD projects tend to be more applied and the topic for them is defined based on an agreement between the PhD advisor and the company. Funding for such projects is either agreed upon with or without a candidate for the position. As with the other two options, the PhD student is enrolled at the Doctoral School.\nMore information on the enrollment process at the Science Faculty Doctoral School is listed here.\nDoing a Postdoc Postdoc positions are available for anything between one and four years. Applicants are expected to have submitted their PhD thesis by the time they start, but it is typically not necessary for them to have been formally awarded a PhD yet, if they have sufficient evidence that this is likely to happen in the near future.\nThe routes to getting a postdoc position are very similar to those for a PhD position:\n applying for a fully-funded position; applying with external funding; applying for an industrial PhD.  Funded postdoc positions are often advertised on the KU job portal, though 1-year postdoc contracts can be offered without open calls. The postdoc topic varies widely based on the funding source.\nIt is, furthermore, possible to start as a postdoc with already obtained funding, e.g. individual research fellowships. The topic for those is typically open and decided by the postdoc candidate. For some fellowship schemes, applications are made jointly with the host. Get in touch if you are interested in this option.\nLastly, industrial postdoc programs means a researcher works both at a university and at a company, roughly one day a week. The postdoc topic is typically more application-oriented and defined based on an agreement between the advisor and the company. Funding is either agreed upon with or without a candidate for the position.\nCopeNLU Group If you arrived on this page, you\u0026rsquo;ve likely already seen the rest of the CopeNLU website. We are a young research group at the Computer Science department at the University of Copenhagen, interested in natural language understanding. We are a very social and collaborative group, with weekly group meetings, breakfasts and reading groups, in addition to one-on-one adviser-advisee meetings. PhD students and postdocs are encouraged to not work on projects on their own, but rather form synergies with other group members based on common research interests.\nVisiting the Group We are always open to short or longer-term visitors to the group, but do not provide internships for Bachelor\u0026rsquo;s or Master\u0026rsquo;s students. Funding for visits can typically not be provided, with the expection of invited speakers who give a talk in the Copenhagen NLP meetup series.\n","date":1549062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549062000,"objectID":"dfe14b7cff1dd6a8394505f419e3da78","permalink":"https://copenlu.github.io/post/why-ucph/","publishdate":"2019-02-02T00:00:00+01:00","relpermalink":"/post/why-ucph/","section":"post","summary":"The University of Copenhagen is a great place if you're both interested in high-quality NLP research and a high quality of life.","tags":["Academic"],"title":"Interested in joining us at the University of Copenhagen?","type":"post"},{"authors":["Johannes Bjerva","Robert Östling","Maria Han Veiga","Jörg Tiedemann","Isabelle Augenstein"],"categories":null,"content":"","date":1548975600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548975600,"objectID":"77c14eca1f87d3b410f72a8c639228e9","permalink":"https://copenlu.github.io/publication/2019_cl_bjerva/","publishdate":"2019-02-01T00:00:00+01:00","relpermalink":"/publication/2019_cl_bjerva/","section":"publication","summary":"A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just like it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, while genetic relationships---a convenient benchmark used for evaluation in previous work---appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.","tags":[],"title":"What do Language Representations Really Represent?","type":"publication"},{"authors":["Sebastian Ruder","Joachim Bingel","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1548889200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548889200,"objectID":"c4469eafb02bfe1292a0bcad561bf951","permalink":"https://copenlu.github.io/publication/2019_aaai_ruder/","publishdate":"2019-01-31T00:00:00+01:00","relpermalink":"/publication/2019_aaai_ruder/","section":"publication","summary":"Multi-task learning (MTL) allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find (a) the layers or subspaces that benefit from sharing, (b) the appropriate amount of sharing, and (c) the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a latent multi-task architecture that jointly addresses (a)--(c). We present experiments on synthetic data and data from OntoNotes 5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15% average error reductions over common approaches to MTL.","tags":[],"title":"Latent multi-task architecture learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1548000421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548000421,"objectID":"95e81310bc3161eac19de3719bf77d94","permalink":"https://copenlu.github.io/people/isabelle/","publishdate":"2019-01-20T17:07:01+01:00","relpermalink":"/people/isabelle/","section":"people","summary":"","tags":["Members"],"title":"Isabelle Augenstein","type":"people"},{"authors":null,"categories":null,"content":"","date":1547914141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547914141,"objectID":"6b65be908a86e77256fbcbf4a3629746","permalink":"https://copenlu.github.io/people/johannes/","publishdate":"2019-01-19T17:09:01+01:00","relpermalink":"/people/johannes/","section":"people","summary":"","tags":["Members"],"title":"Johannes Bjerva","type":"people"},{"authors":null,"categories":null,"content":"","date":1547914021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547914021,"objectID":"9dec93e26a79a922ecb78073404d2edb","permalink":"https://copenlu.github.io/people/pepa/","publishdate":"2019-01-19T17:07:01+01:00","relpermalink":"/people/pepa/","section":"people","summary":"","tags":["Members"],"title":"Pepa Atanasova","type":"people"},{"authors":null,"categories":null,"content":"","date":1547482021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547482021,"objectID":"7d63205eaae7699bf4c5872340c0fcb8","permalink":"https://copenlu.github.io/people/andreas/","publishdate":"2019-01-14T17:07:01+01:00","relpermalink":"/people/andreas/","section":"people","summary":"","tags":["Members"],"title":"Andreas Nugaard Holm","type":"people"},{"authors":null,"categories":null,"content":"","date":1547482021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547482021,"objectID":"77c3ada3dcc99d95c6734f9bf4ac0757","permalink":"https://copenlu.github.io/people/dustin/","publishdate":"2019-01-14T17:07:01+01:00","relpermalink":"/people/dustin/","section":"people","summary":"","tags":["Members"],"title":"Dustin Wright","type":"people"},{"authors":null,"categories":null,"content":"","date":1547395741,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547395741,"objectID":"a546055b1943e6eddf28c042caf2d888","permalink":"https://copenlu.github.io/people/oscar/","publishdate":"2019-01-13T17:09:01+01:00","relpermalink":"/people/oscar/","section":"people","summary":"","tags":["Members"],"title":"Oscar Kjell","type":"people"},{"authors":["Ana V. González-Garduño ","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1540940400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540940400,"objectID":"7be805f652a43c8d17f6d8767fc0c080","permalink":"https://copenlu.github.io/publication/2018_emnlp_gonzalez/","publishdate":"2018-10-31T00:00:00+01:00","relpermalink":"/publication/2018_emnlp_gonzalez/","section":"publication","summary":"The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks -- a task that amounts to question relevancy ranking -- involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google's search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.","tags":[],"title":"A strong baseline for question relevancy ranking","type":"publication"},{"authors":["Miryam de Lhoneux","Johannes Bjerva","Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1540854000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540854000,"objectID":"11a2fc1d52386ca7a3d13ca4b2fea67f","permalink":"https://copenlu.github.io/publication/2018_emnlp_de-lhoneux/","publishdate":"2018-10-30T00:00:00+01:00","relpermalink":"/publication/2018_emnlp_de-lhoneux/","section":"publication","summary":"Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a parser on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.","tags":[],"title":"Parameter sharing between dependency parsers for related languages","type":"publication"},{"authors":["Yova Kementchedjhieva","Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"1b3f9646468795b7081d3cd891e790cd","permalink":"https://copenlu.github.io/publication/2018_sigmorphon_kementchedjhieva/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/2018_sigmorphon_kementchedjhieva/","section":"publication","summary":"This paper documents the Team Copenhagen system which placed first in the CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the 'inflection in context' task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.","tags":[],"title":" Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding","type":"publication"},{"authors":["Anders Søgaard","Miryam de Lhoneux","Isabelle Augenstein"],"categories":null,"content":"","date":1538258400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538258400,"objectID":"f3e4355ecd1db9a9ea4bfb28d4775004","permalink":"https://copenlu.github.io/publication/2018_blackbox_soegaard/","publishdate":"2018-09-30T00:00:00+02:00","relpermalink":"/publication/2018_blackbox_soegaard/","section":"publication","summary":"Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal. Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation. We also use punctuation ungrammatically for emphatic or creative purposes, or simply by mistake. We show that (a) dependency parsers are sensitive to both absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers without punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation. Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data.","tags":[],"title":"Nightmare at test time: How punctuation prevents parsers from generalizing","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://copenlu.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Dirk Weissenborn","Pasquale Minervini","Tim Dettmers","Isabelle Augenstein","Johannes Welbl","Tim Rocktäschel","Matko Bošnjak","Jeff Mitchell","Thomas Demeester","Pontus Stenetorp","Sebastian Riedel"],"categories":null,"content":"","date":1533074400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533074400,"objectID":"30a8402ff65b94561c394b6a22ec12f2","permalink":"https://copenlu.github.io/publication/2018_acl_weissenborn/","publishdate":"2018-08-01T00:00:00+02:00","relpermalink":"/publication/2018_acl_weissenborn/","section":"publication","summary":"Many Machine Reading and Natural Language Understanding tasks require reading supporting text in order to answer questions. For example, in Question Answering, the supporting text can be newswire or Wikipedia articles; in Natural Language Inference, premises can be seen as the supporting text and hypotheses as questions. Providing a set of useful primitives operating in a single framework of related tasks would allow for expressive modelling, and easier model comparison and replication. To that end, we present Jack the Reader (Jack), a framework for Machine Reading that allows for quick model prototyping by component reuse, evaluation of new models on existing datasets as well as integrating new datasets and applying them on a growing set of implemented baseline models. Jack is currently supporting (but not limited to) three tasks: Question Answering, Natural Language Inference, and Link Prediction. It is developed with the aim of increasing research efficiency and code reuse.","tags":[],"title":"Jack the Reader – A Machine Reading Framework","type":"publication"},{"authors":["Katharina Kann","Johannes Bjerva","Isabelle Augenstein","Barbara Plank","Anders Søgaard"],"categories":null,"content":"","date":1532988000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532988000,"objectID":"ecddfe04d32ca116b139cbaae21fda6c","permalink":"https://copenlu.github.io/publication/2018_deeplo_kann/","publishdate":"2018-07-31T00:00:00+02:00","relpermalink":"/publication/2018_deeplo_kann/","section":"publication","summary":"Neural part-of-speech (POS) taggers are known to not perform well with little training data. As a step towards overcoming this problem, we present an architecture for learning more robust neural POS taggers by jointly training a hierarchical, recurrent model and a recurrent characterbased sequence-to-sequence network supervised using an auxiliary objective. This way, we introduce stronger character-level supervision into the model, which enables better generalization to unseen words and provides regularization, making our encoding less prone to overfitting. We experiment with three auxiliary tasks: lemmatization, character-based word autoencoding, and character-based random string autoencoding. Experiments with minimal amounts of labeled data on 34 languages show that our new architecture outperforms a single-task baseline and, surprisingly, that, on average, raw text autoencoding can be as beneficial for lowresource POS tagging as using lemma information. Our neural POS tagger closes the gap to a state-of-the-art POS tagger (MarMoT) for low-resource scenarios by 43%, even outperforming it on languages with templatic morphology, e.g., Arabic, Hebrew, and Turkish, by some margin","tags":[],"title":"Character-level Supervision for Low-resource POS Tagging","type":"publication"},{"authors":["Isabelle Augenstein","Kris Cao","He He","Felix Hill","Spandana Gella","Jamie Kiros","Hongyuan Mei","Dipendra Misra"],"categories":null,"content":"","date":1532988000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532988000,"objectID":"6e6ca8c5d9f158ee98eede711a95f30f","permalink":"https://copenlu.github.io/publication/2018_repl4nlp_augenstein/","publishdate":"2018-07-31T00:00:00+02:00","relpermalink":"/publication/2018_repl4nlp_augenstein/","section":"publication","summary":"The ACL 2018 Workshop on Representation Learning for NLP (RepL4NLP) takes place on Friday, July 20, 2018 in Melbourne, Australia, immediately following the 56th Annual Meeting of the Association for Computational Linguistics (ACL). The workshop is generously sponsored by Facebook, Salesforce, ASAPP, DeepMind, Microsoft Research, and Naver. Repl4NLP is organised by Isabelle Augenstein, Kris Cao, He He, Felix Hill, Spandana Gella, Jamie Kiros, Hongyuan Mei and Dipendra Misra, and advised by Kyunghyun Cho, Edward Grefenstette, Karl Moritz Hermann and Laura Rimell. The 3rd Workshop on Representation Learning for NLP aims to continue the success of the 1st Workshop on Representation Learning for NLP, which received about 50 submissions and over 250 attendees and was the second most attended collocated event at ACL 2016 in Berlin, Germany after WMT; and the 2nd Workshop on Representation Learning for NLP at ACL 2017 in Vancouver, Canada. The workshop has a focus on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP.","tags":[],"title":"Proceedings of The Third Workshop on Representation Learning for NLP","type":"publication"},{"authors":["Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"04c9a44363b1ede24a1b169269f632e4","permalink":"https://copenlu.github.io/publication/2018_naacl_bjerva/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_bjerva/","section":"publication","summary":"A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task. For instance, although Norwegian Bokmal and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. We are also able to predict typological features in WALS with high accuracies, even for unseen language families.","tags":[],"title":"From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings","type":"publication"},{"authors":["Isabelle Augenstein","Sebastian Ruder","Anders Søgaard"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"59e919526653ad514493d33dd8423a61","permalink":"https://copenlu.github.io/publication/2018_naacl_augenstein/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_augenstein/","section":"publication","summary":"We combine multi-task learning and semisupervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis.","tags":[],"title":"Multi-Task Learning of Pairwise Sequence Classification Tasks over      Disparate Label Spaces","type":"publication"},{"authors":["Thomas Nyegaard-Signori","Casper Veistrup Helms","Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1527804000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527804000,"objectID":"d1aafea6345c3797374bf6e984b97818","permalink":"https://copenlu.github.io/publication/2018_naacl_nyegaard-signori/","publishdate":"2018-06-01T00:00:00+02:00","relpermalink":"/publication/2018_naacl_nyegaard-signori/","section":"publication","summary":"We take a multi-task learning approach to the shared Task 1 at SemEval-2018. The general idea concerning the model structure is to use as little external data as possible in order to preserve the task relatedness and reduce complexity. We employ multi-task learning with hard parameter sharing to exploit the relatedness between sub-tasks. As a base model, we use a standard recurrent neural network for both the classification and regression subtasks. Our system ranks 32nd out of 48 participants with a Pearson score of 0.557 in the first subtask, and 20th out of 35 in the fifth subtask with an accuracy score of 0.464.","tags":[],"title":"KU-MTL at SemEval-2018 Task 1: Multi-task Identification of Affect in Tweets","type":"publication"},{"authors":["Arkaitz Zubiaga","Elena Kochkina","Maria Liakata","Rob Procter","Michal Lukasik","Kalina Bontcheva","Trevor Cohn","Isabelle Augenstein"],"categories":null,"content":"","date":1519858800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519858800,"objectID":"16c50ca2bb654ef1b5375468d0a6cd8a","permalink":"https://copenlu.github.io/publication/2018_ipm_zubiaga/","publishdate":"2018-03-01T00:00:00+01:00","relpermalink":"/publication/2018_ipm_zubiaga/","section":"publication","summary":"Rumour stance classification, defined as classifying the stance of specific social media posts into one of supporting, denying, querying or commenting on an earlier post, is becoming of increasing interest to researchers. While most previous work has focused on using individual tweets as classifier inputs, here we report on the performance of sequential classifiers that exploit the discourse features inherent in social media interactions or 'conversational threads'. Testing the effectiveness of four sequential classifiers -- Hawkes Processes, Linear-Chain Conditional Random Fields (Linear CRF), Tree-Structured Conditional Random Fields (Tree CRF) and Long Short Term Memory networks (LSTM) -- on eight datasets associated with breaking news stories, and looking at different types of local and contextual features, our work sheds new light on the development of accurate stance classifiers. We show that sequential classifiers that exploit the use of discourse properties in social media conversations while using only local features, outperform non-sequential classifiers. Furthermore, we show that LSTM using a reduced set of features can outperform the other sequential classifiers; this performance is consistent across datasets and across types of stances. To conclude, our work also analyses the different features under study, identifying those that best help characterise and distinguish between stances, such as supporting tweets being more likely to be accompanied by evidence than denying tweets. We also set forth a number of directions for future research.","tags":[],"title":"Discourse-Aware Rumour Stance Classification in Social Media Using Sequential Classifiers","type":"publication"},{"authors":null,"categories":null,"content":"","date":1516550821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516550821,"objectID":"28b1f1dedf78ca52283f0864b4703960","permalink":"https://copenlu.github.io/people/ryan/","publishdate":"2018-01-21T17:07:01+01:00","relpermalink":"/people/ryan/","section":"people","summary":"","tags":["Affiliated"],"title":"Ryan Cotterell","type":"people"},{"authors":null,"categories":null,"content":"","date":1516464421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516464421,"objectID":"aa7eb934e88b2941f7b2ea1dbd8d2b9a","permalink":"https://copenlu.github.io/people/yova/","publishdate":"2018-01-20T17:07:01+01:00","relpermalink":"/people/yova/","section":"people","summary":"","tags":["Affiliated"],"title":"Yova Kementchedjhieva","type":"people"},{"authors":null,"categories":null,"content":"","date":1516378021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516378021,"objectID":"2aa98cf9e7d8b01f62de357efeda8999","permalink":"https://copenlu.github.io/people/ana/","publishdate":"2018-01-19T17:07:01+01:00","relpermalink":"/people/ana/","section":"people","summary":"","tags":["Affiliated"],"title":"Ana Valeria González","type":"people"},{"authors":null,"categories":null,"content":"","date":1516205221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516205221,"objectID":"3b84fd5ae50359e916a4a0606047cdd9","permalink":"https://copenlu.github.io/people/andrea/","publishdate":"2018-01-17T17:07:01+01:00","relpermalink":"/people/andrea/","section":"people","summary":"","tags":["Affiliated"],"title":"Andrea Lekkas","type":"people"},{"authors":null,"categories":null,"content":"","date":1516118821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516118821,"objectID":"3b4f9852a4e8545215604892dafb43d8","permalink":"https://copenlu.github.io/people/nils/","publishdate":"2018-01-16T17:07:01+01:00","relpermalink":"/people/nils/","section":"people","summary":"","tags":["Affiliated"],"title":"Nils Rethmeier","type":"people"},{"authors":null,"categories":null,"content":"","date":1516032421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516032421,"objectID":"ba2e9dcf3803a2e7ce35513d039cffd2","permalink":"https://copenlu.github.io/people/ran/","publishdate":"2018-01-15T17:07:01+01:00","relpermalink":"/people/ran/","section":"people","summary":"","tags":["Affiliated"],"title":"Ran Zmigrod","type":"people"},{"authors":["Johannes Bjerva","Isabelle Augenstein"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"375f4d44665380f3a0c98378eb3e263e","permalink":"https://copenlu.github.io/publication/2018_iwclul_bjerva/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/2018_iwclul_bjerva/","section":"publication","summary":"*English Abstract*: Although linguistic typology has a long history, computational approaches have only recently gained popularity. The use of distributed representations in computational linguistics has also become increasingly popular. A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another. Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing. In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks. We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations. We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline. *Finnish Abstract*: Vaikka kielitypologialla on pitkä historia, siihen liittyvät laskennalliset menetelmät ovat vasta viime aikoina saavuttaneet suosiota. Myös hajautettujen representaatioiden käyttö laskennallisessa kielitieteessä on tullut yhä suositummaksi. Viimeaikainen kehitys alalla on oppia kielestä hajautettu representaatio, joka esittää samankaltaiset kielet lähellä toisiaan. Vaikka kyseiset representaatiot nauttivatkin empiiristä menestystä, ei niitä ole huomattavasti tutkittu typologisesti. Tässä artikkelissa tutkitaan, ovatko tällaiset kielirepresentaatiot empiirisesti käyttökelpoisia uralilaisten kielten välisissä mallimuunnoksissa syvissä neuroverkoissa. Pyrkimällä ennustamaan piirteitä \textit{World Atlas of Language Structures}-tietokannassa tutkimme, mitä typologisia ominaisuuksia nämä representaatiot sisältävät. Keskityimme uralilaisiin kieliin ja huomasimme, että jotkin typologiset ominaisuudet voidaan automaattisesti päätellä tarkkuudella, joka ylittää selvästi vahvan perustason.","tags":[],"title":"Tracking Typological Traits of Uralic Languages in Distributed Language Representations","type":"publication"},{"authors":["Ed Collins","Isabelle Augenstein","Sebastian Riedel"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"7551d306de802e85bed4c73da927408b","permalink":"https://copenlu.github.io/publication/2017_conll_collins/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_conll_collins/","section":"publication","summary":"Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.","tags":[],"title":"A Supervised Approach to Extractive Summarisation of Scientific Papers","type":"publication"},{"authors":["Benjamin Riedel","Isabelle Augenstein","Georgios Spithourakis","Sebastian Riedel"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"2ceef4e10df7fe69e161e122c1289476","permalink":"https://copenlu.github.io/publication/2017_arxiv_riedel/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_arxiv_riedel/","section":"publication","summary":"Identifying public misinformation is a complicated and challenging task. An important part of checking the veracity of a specific claim is to evaluate the stance different news sources take towards the assertion. Automatic stance evaluation, i.e. stance detection, would arguably facilitate the process of fact checking. In this paper, we present our stance detection system which claimed third place in Stage 1 of the Fake News Challenge. Despite our straightforward approach, our system performs at a competitive level with the complex ensembles of the top two winning teams. We therefore propose our system as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance detection task.","tags":[],"title":"A simple but tough-to-beat baseline for the Fake News Challenge stance detection task","type":"publication"},{"authors":["Isabelle Augenstein","Leon Derczynski","Kalina Bontcheva"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"7687b70fc2497295c5eadce83e18dbcc","permalink":"https://copenlu.github.io/publication/2017_ipm_augenstein/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_ipm_augenstein/","section":"publication","summary":"Named Entity Recognition (NER) is a key NLP task, which is all the more challenging on Web and user-generated content with their diverse and continuously changing language. This paper aims to quantify how this diversity impacts state-of-the-art NER methods, by measuring named entity (NE) and context variability, feature sparsity, and their effects on precision and recall. In particular, our findings indicate that NER approaches struggle to generalise in diverse genres with limited training data. Unseen NEs, in particular, play an important role, which have a higher incidence in diverse genres such as social media than in more regular genres such as newswire. Coupled with a higher incidence of unseen features more generally and the lack of large training corpora, this leads to significantly lower F1 scores for diverse genres as compared to more regular ones. We also find that leading systems rely heavily on surface forms found in training data, having problems generalising beyond these, and offer explanations for this observation.","tags":[],"title":"Generalisation in Named Entity Recognition: A Quantitative Analysis","type":"publication"},{"authors":["Isabelle Augenstein","Anders Søgaard"],"categories":null,"content":"","date":1498860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498860000,"objectID":"531bd207fb677a26642fb1f0fb61a1a0","permalink":"https://copenlu.github.io/publication/2017_acl_augenstein/","publishdate":"2017-07-01T00:00:00+02:00","relpermalink":"/publication/2017_acl_augenstein/","section":"publication","summary":"Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.","tags":[],"title":"Multi-Task Learning of Keyphrase Boundary Classification","type":"publication"},{"authors":["Isabelle Augenstein","Mrinal Das","Sebastian Riedel","Lakshmi Vikraman","Andrew McCallum"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"ac64c02d198f19b94c38dd746743353a","permalink":"https://copenlu.github.io/publication/2017_semeval_augenstein/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_semeval_augenstein/","section":"publication","summary":"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.","tags":[],"title":"SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications","type":"publication"},{"authors":["Elena Kochkina","Maria Liakata","Isabelle Augenstein"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"feb9b0fc3720c6ab93685d96db50c2f4","permalink":"https://copenlu.github.io/publication/2017_winlp_kochkina/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_winlp_kochkina/","section":"publication","summary":"Rumour stance classification is a task that involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose an LSTM-based sequential model that, through modelling the conversational structure of tweets, obtains state-of-theart accuracy on the SemEval-2017 RumourEval dataset.","tags":[],"title":"Sequential Approach to Rumour Stance Classification","type":"publication"},{"authors":["Elena Kochkina","Maria Liakata","Isabelle Augenstein"],"categories":null,"content":"","date":1498773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498773600,"objectID":"420b4382fe0eafec280fa592bc2a70b4","permalink":"https://copenlu.github.io/publication/2017_semeval_kochkina/","publishdate":"2017-06-30T00:00:00+02:00","relpermalink":"/publication/2017_semeval_kochkina/","section":"publication","summary":"This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.","tags":[],"title":"Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM","type":"publication"},{"authors":["Ziqi Zhang","Anna Lisa Gentile","Isabelle Augenstein","Eva Blomqvist","Fabio Ciravegna"],"categories":null,"content":"","date":1488322800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488322800,"objectID":"48dc2495a1f6b9cba1bcd037aed41069","permalink":"https://copenlu.github.io/publication/2017_swj_zhang/","publishdate":"2017-03-01T00:00:00+01:00","relpermalink":"/publication/2017_swj_zhang/","section":"publication","summary":"The Web of Data is currently undergoing an unprecedented level of growth thanks to the Linked Open Data effort. One escalated issue is the increasing level of heterogeneity in the published resources. This seriously hampers interoperability of Semantic Web applications. A decade of effort in the research of Ontology Alignment has contributed to a rich literature to solve such problems. However, existing methods can be still limited as 1) they primarily address concepts and entities while relations are less well-studied; 2) many build on the assumption of the ‘well-formedness’ of ontologies which is unnecessarily true in the domain of Linked Open Data; 3) few looked at schema heterogeneity from a single source, which is also a common issue particularly in very large Linked Dataset created automatically from heterogeneous resources, or integrated from multiple datasets. This article aims to address these issues with a domain- and language-independent and completely unsupervised method to align equivalent relations across schemata based on their shared instances. We propose a novel similarity measure able to cope with unbalanced population of schema elements, an unsupervised technique to automatically decide similarity threshold to assert equivalence for a pair of relations, and an unsupervised clustering process to discover groups of equivalent relations across different schemata. Although the method is designed for aligning relations within a single dataset, it can also be adapted for cross-dataset alignment where sameAs links between datasets have been established. Using three gold standards created based on DBpedia, we obtain encouraging results from a thorough evaluation involving four baseline similarity measures and over 15 comparative models based on variants of the proposed method. The proposed method makes significant improvement over baseline models in terms of F1 measure (mostly between 7% and 40%), and it always scores the highest precision and is also among the top performers in terms of recall. We also make public the datasets used in this work, which we believe make the largest collection of gold standards for evaluating relation alignment in the LOD context.","tags":[],"title":"An Unsupervised Data-driven Method to Discover Equivalent Relations in Large Linked Datasets","type":"publication"},{"authors":null,"categories":null,"content":"","date":1484928421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484928421,"objectID":"c773ea7fff41710544da3ebb32d508e2","permalink":"https://copenlu.github.io/people/mareike/","publishdate":"2017-01-20T17:07:01+01:00","relpermalink":"/people/mareike/","section":"people","summary":"","tags":["Alumni"],"title":"Mareike Hartmann","type":"people"},{"authors":null,"categories":null,"content":"","date":1484928421,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484928421,"objectID":"0f90039fbe3f22579f88378edf717cc9","permalink":"https://copenlu.github.io/people/wei/","publishdate":"2017-01-20T17:07:01+01:00","relpermalink":"/people/wei/","section":"people","summary":"","tags":["Members"],"title":"Wei Zhao","type":"people"},{"authors":null,"categories":null,"content":"","date":1484842021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484842021,"objectID":"4a0551a8f7326bd863d21e9e474d8410","permalink":"https://copenlu.github.io/people/farhad/","publishdate":"2017-01-19T17:07:01+01:00","relpermalink":"/people/farhad/","section":"people","summary":"","tags":["Alumni"],"title":"Farhad Nooralahzadeh","type":"people"},{"authors":null,"categories":null,"content":"","date":1484842021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484842021,"objectID":"b96bcc0a756b3a2e3e6a041824ced7fb","permalink":"https://copenlu.github.io/people/xuan/","publishdate":"2017-01-19T17:07:01+01:00","relpermalink":"/people/xuan/","section":"people","summary":"","tags":["Alumni"],"title":"Zhong Xuan","type":"people"},{"authors":null,"categories":null,"content":"","date":1484755621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484755621,"objectID":"8f7ec6df3ddaddd23752a710340e1517","permalink":"https://copenlu.github.io/people/luna/","publishdate":"2017-01-18T17:07:01+01:00","relpermalink":"/people/luna/","section":"people","summary":"","tags":["Alumni"],"title":"Luna De Bruyne","type":"people"},{"authors":null,"categories":null,"content":"","date":1484669221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484669221,"objectID":"5e82376c2db939241c73c98ea3326c8b","permalink":"https://copenlu.github.io/people/giannis/","publishdate":"2017-01-17T17:07:01+01:00","relpermalink":"/people/giannis/","section":"people","summary":"","tags":["Alumni"],"title":"Giannis Bekoulis","type":"people"},{"authors":null,"categories":null,"content":"","date":1484582941,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484582941,"objectID":"aa55b41abc8e2441b932f86bfd7e67b1","permalink":"https://copenlu.github.io/people/sune/","publishdate":"2017-01-16T17:09:01+01:00","relpermalink":"/people/sune/","section":"people","summary":"","tags":["Alumni"],"title":"Sune Debel","type":"people"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"4acb79057f3b889f1ad90825bccc8b36","permalink":"https://copenlu.github.io/talk_backup/example/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/talk_backup/example/","section":"talk_backup","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk_backup"},{"authors":null,"categories":null,"content":"Learning with limited labelled data is useful for small domains or languages with little resources. Methods we research to mitigate problems arising in these contexts include multi-task learning, weakly supervised and zero-shot learning.\nThis is a cross-cutting theme in most of our research. Two funded projects specifically addressing this are Multi3Generation and Andreas Nugaard Holm\u0026rsquo;s industrial PhD project with BASE Life Science, supported by Innovation Fund Denmark.\nMulti3Generation is a COST Action that funds collaboration of researchers in Europe and abroad. The project is coordinated by Isabelle Augenstein, and its goals are to study language generation using multi-task, multilingual and multi-modal signals.\nAndreas Nugaard Holm\u0026rsquo;s industrial PhD project focuses on transfer learning and domain adaptation for scientific text.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"673da02c015e56b1ada0bfcd1fc5431c","permalink":"https://copenlu.github.io/project/limited-data/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/limited-data/","section":"project","summary":"Learning with limited labelled data, including multi-task learning, weakly supervised and zero-shot learning","tags":["lld","limited-data"],"title":"Learning with Limited Labelled Data","type":"project"},{"authors":null,"categories":null,"content":"Multi-lingual learning is concerned with training models to work well for multiple languages, including low-resource ones. We research methods for enabling information sharing between multiple languages, and also study how to utilise typological knowledge bases to this end. We are currently involved in three larger funded projects on this.\nMulti3Generation is a COST Action that funds collaboration of researchers in Europe and abroad. The project is coordinated by Isabelle Augenstein, and its goals are to study language generation using multi-task, multilingual and multi-modal signals.\nWe are further partner in a research project funded by the Swedish Research Council coordinated by Robert Östling. Its goals are to study structured multilinguality, i.e. the idea of using language representations and typological knowledge bases to guide which information to share between specific languages.\nLastly, Andrea Lekkas\u0026rsquo; industrial PhD project with Ordbogen, supported by Innovation Fund Denmark, focuses on multilingual language modelling for developing writing assistants.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"c8405c441be49e9303825023c78eeadf","permalink":"https://copenlu.github.io/project/multilingual-learning/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/multilingual-learning/","section":"project","summary":"Training models to work well for multiple languages, including low-resource ones","tags":["lld","multilingual-learning"],"title":"Multilingual Learning","type":"project"},{"authors":null,"categories":null,"content":"Question answering is concerned with answer user questions, either in a closed-domain or open-domain setting automatically. We are interested in exploiting synergies between question answering and related tasks, such as framing entailment or relation extraction as question answering tasks. Further research interests are question answering in conversational settings, such as for chatbots.\nWe are currently involved in two funded projects related to this theme: a project on subjectivity in question answering, funded by a faculty research award from Megagon Labs; as well a PhD fellowship grant on conversational question answering for customer support from BotXO, which Ana Valeria Gonzalez works on.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"464368a48f0c4982ed7a6e6c606693ca","permalink":"https://copenlu.github.io/project/question-answering/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/question-answering/","section":"project","summary":"Answering questions automatically, including in conversational settings","tags":["nlu","question-answering"],"title":"Question Answering","type":"project"},{"authors":null,"categories":null,"content":"We are interested in studying method to determine the attitude expressed in a text towards a topic (stance detection), such as determining if a tweet expresses a positive, negative or neutral stance towards a political entity. One additional challenge we are exploring is stance detection in a conversational context, where the stance depends on the context of the conversation. Fact checking using textual data can be framed very similarly, namely as if an evidence document agrees with, disagrees with or is topically unrelated to a headline or claim.\nWe are also researching the relationship between attitudes towards entities on social media and gender bias as part of a project funded by DFF.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"9b262f5564a2254298043b81d48f5aa5","permalink":"https://copenlu.github.io/project/fact-checking/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/fact-checking/","section":"project","summary":"Determine the attitude expressed in a text towards a topic, and use this for automatic evidence-based fact checking","tags":["nlu","fact-checking"],"title":"Stance Detection and Fact Checking","type":"project"},{"authors":null,"categories":null,"content":"We are working on studying methods to detect gendered language automatically using unsupervised learning methods, such as variational auto-encoders. The findings of our first paper on this (Hoyle et al., 2019) have been reported by 75+ international news outlets, including Forbes.\nCurrently, we\u0026rsquo;re interested in expanding the above to a cross-lingual study, as well as researching the relationship between gender bias and attitudes towards entities on social media as part of a project funded by DFF.\n","date":1461621600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461621600,"objectID":"f488819b45376aa4eb59e4e045239e20","permalink":"https://copenlu.github.io/project/gender-bias/","publishdate":"2016-04-26T00:00:00+02:00","relpermalink":"/project/gender-bias/","section":"project","summary":"Automatically detecting gendered language, and to what degree attitudes towards entities are influenced by gender bias","tags":["nlu","gender-bias"],"title":"Gender Bias Detection","type":"project"},{"authors":null,"categories":null,"content":"Information extraction is concerned with extracting information about entities, phrases and relations between them from text to populate knowledge bases, such as extracting \u0026ldquo;employee-at\u0026rdquo; relations. Within this context, we have worked on automatic knowledge base completion, knowledge base cleansing and detecting scientific keyphrases in text, as well as automatic completion of typological knowledge bases.\nWe are currently involved in one longer-term project related to this, namely a research project funded by the Swedish Research Council coordinated by Robert Östling. Its goals are to study structured multilinguality, i.e. the idea of using language representations and typological knowledge bases to guide which information to share between specific languages.\n","date":1459033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459033200,"objectID":"dbf6be1f58ca6227cd1a2b0d1e198a14","permalink":"https://copenlu.github.io/project/knowledge-bases/","publishdate":"2016-03-27T00:00:00+01:00","relpermalink":"/project/knowledge-bases/","section":"project","summary":"Extract information about entities, phrases and relations between them from text to populate knowledge bases","tags":["nlu","knowledge-bases"],"title":"Knowledge Base Population","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441058400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441058400,"objectID":"02ddcc34f9b380308becfdd12994b2f4","permalink":"https://copenlu.github.io/publication_backup/person-re-id/","publishdate":"2015-09-01T00:00:00+02:00","relpermalink":"/publication_backup/person-re-id/","section":"publication_backup","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication_backup"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372629600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372629600,"objectID":"0af71cf2c5a7a78c0b9a5906512f8538","permalink":"https://copenlu.github.io/publication_backup/clothing-search/","publishdate":"2013-07-01T00:00:00+02:00","relpermalink":"/publication_backup/clothing-search/","section":"publication_backup","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication_backup"}]